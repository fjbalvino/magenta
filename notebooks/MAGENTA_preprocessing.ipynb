{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d115eb9",
      "metadata": {
        "id": "8d115eb9"
      },
      "source": [
        "# **MAGENTA:** The Global **MA**ngrove **GEN**e Ca**TA**logue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee10325d",
      "metadata": {
        "id": "ee10325d"
      },
      "source": [
        "---\n",
        "# SUMMARY\n",
        "\n",
        "Los manglares son una conocida reserva de diversidad biológica y un ecosistema altamente productivo. Diversos estudios metagenómicos en diferentes partes del mundo han reconocido a la comunidad microbiana del manglar como un agente importante dentro de los ciclos biogeoquímicos, en los cuales se llevan a cabo procesos tales como la transformación del carbono, la fotosíntesis, la fijación de nitrógeno y la reducción de azufre. En la actualidad, sin embargo, no contamos con una herramienta informática que nos permita entender estos procesos y relaciones a una **escala global**.\n",
        "MAGENTA (o Global MAngrove GENe CaTAlogue) actúa como un catálogo global de genes únicos y no redundantes a nivel de especie (agrupados al 95% de identidad de nucleótidos) que, a partir de los datos de libre acceso disponibles en bases de datos especializadas (WGS, metagenomas de acceso público - ENA) de cinco de los principales hábitats de la comunidad microbiana del manglar (rizosfera, agua de mar, sedimento, suelo, humedal), busca formular nuevas hipótesis sobre la abundancia, distribución y funciones metabólicas de los microorganismos en el ecosistema del manglar, con miras a atender esta necesidad.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be97814a",
      "metadata": {
        "id": "be97814a"
      },
      "source": [
        "# MAGENTA — descarga y enriquecimiento de metadatos\n",
        "\n",
        "El contexto de MAGENTA\n",
        "\n",
        "Al integrar metagenomas públicos, incorporamos sesgos de curación heterogénea (especialmente en NCBI). Usando coordenadas como eje común, incorporamos metadatos de repositorios globales y de acceso publico. Su contexto ambiental explicito altitud, clima (BIO1–BIO19), ecorregión marina, huella humana, tipo de suelos, pH, secuestro de carbono, cobertura vegetal (canopy), y diversidad de especies, sitúan a cada metadato en su nicho ambiental.  Esta estandarización de metadatos permite modelos comparativos y reproducibles, con el potencial de generar irmformación util en el desarrollo de estrategias de conservación, restauración, y la generación de hipótesis funcionales\n",
        "\n",
        "## Reproducibilidad y entorno\n",
        "\n",
        "- Fecha de generación del _notebook_: **2025-08-11**\n",
        "- Requisitos (sugeridos): `python>=3.10`, `pandas`, `requests`, `tqdm`, `rasterio`, `geopandas`, `numpy`, `elevation`, `biopython`, `entrez-direct`, `sra-tools`, `aria2c`/`wget`, opcionalmente `aspera`.\n",
        "- Sistema de archivos: el flujo se diseñó para coexistir con dos estilos de rutas previamente usadas en MAGENTA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c68676e0",
      "metadata": {
        "id": "c68676e0"
      },
      "source": [
        "## 0) Estandarización de rutas y configuración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc1a727",
      "metadata": {
        "id": "cdc1a727"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import shlex\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from urllib.parse import urlencode\n",
        "from tqdm import tqdm\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import rasterio\n",
        "from rasterio.warp import transform as rio_transform\n",
        "\n",
        "# MAGENTA_DIR: respeta env externo; si no, usa el por defecto\n",
        "os.environ.setdefault(\"MAGENTA_DIR\", \"/nfs/testing/.jbalvino/MAGENTA/MAGENTA_DIR\")\n",
        "PROJECT_DIR = Path(os.environ.get(\"MAGENTA_DIR\", Path.cwd())).resolve()\n",
        "\n",
        "RAW_DIR  = PROJECT_DIR / \"rawdata\" / \"fastq\"\n",
        "META_DIR = PROJECT_DIR / \"metadata\"\n",
        "OUT_DIR  = PROJECT_DIR / \"outputs\"\n",
        "LOGS_DIR = PROJECT_DIR / \"logs\"\n",
        "AUX_DIR  = PROJECT_DIR / \"aux\"\n",
        "for d in [RAW_DIR, META_DIR, OUT_DIR, LOGS_DIR, AUX_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "META_BASE = \"metadatos_manglar\"\n",
        "META_FILE = OUT_DIR / f\"{META_BASE}.csv\"\n",
        "\n",
        "def save_tsv(df: pd.DataFrame, path: Path):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(path, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_tsv(path: Path) -> pd.DataFrame:\n",
        "    return pd.read_csv(path, sep=\"\\t\", dtype=str)\n",
        "\n",
        "def ensure_float(df: pd.DataFrame, cols):\n",
        "    for c in cols:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def km_to_deg(km: float) -> float:\n",
        "    return km / 111.32\n",
        "\n",
        "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
        "\n",
        "# --- Entorno seguro para curl/EDirect y requests ---\n",
        "CA_BUNDLE = certifi.where()\n",
        "SAFE_ENV = dict(os.environ)\n",
        "SAFE_ENV[\"CURL_CA_BUNDLE\"] = CA_BUNDLE\n",
        "SAFE_ENV[\"SSL_CERT_FILE\"]  = CA_BUNDLE\n",
        "SAFE_ENV.setdefault(\"TERM\", \"dumb\")\n",
        "SAFE_ENV.setdefault(\"EUTILS_TOOL\", \"magenta_edirect\")\n",
        "SAFE_ENV.setdefault(\"EUTILS_EMAIL\", \"jbalvino@masternew\")\n",
        "os.environ[\"REQUESTS_CA_BUNDLE\"] = CA_BUNDLE\n",
        "\n",
        "def run_cmd(cmd: str, retries: int = 3) -> int:\n",
        "    print(\">>\", cmd)\n",
        "    last = 1\n",
        "    for i in range(1, retries+1):\n",
        "        last = subprocess.call(cmd, shell=True, env=SAFE_ENV)\n",
        "        if last == 0:\n",
        "            return 0\n",
        "        print(f\"[run_cmd][retry {i}/{retries}] exit={last}\")\n",
        "    return last\n",
        "\n",
        "def have_edirect() -> bool:\n",
        "    for tool in (\"esearch\", \"efetch\"):\n",
        "        code = subprocess.call(f\"command -v {tool} >/dev/null 2>&1\", shell=True, env=SAFE_ENV)\n",
        "        if code != 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def postfilter_wgs_illumina_csv(csv_in: Path, csv_out: Path):\n",
        "    \"\"\"Filtra RunInfo por LibraryStrategy=WGS y Platform=ILLUMINA (case-insensitive).\"\"\"\n",
        "    if not csv_in.exists() or csv_in.stat().st_size == 0:\n",
        "        return\n",
        "    with open(csv_in, newline='', encoding='utf-8', errors=\"ignore\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        rows = list(reader)\n",
        "        if not rows:\n",
        "            csv_out.write_text(\"\") ; return\n",
        "        fields = reader.fieldnames\n",
        "    def ok(row):\n",
        "        ls = (row.get(\"LibraryStrategy\") or \"\").upper()\n",
        "        pf = (row.get(\"Platform\") or \"\").upper()\n",
        "        return (\"WGS\" in ls) and (\"ILLUMINA\" in pf)\n",
        "    kept = [r for r in rows if ok(r)]\n",
        "    with open(csv_out, \"w\", newline='', encoding='utf-8') as g:\n",
        "        w = csv.DictWriter(g, fieldnames=fields)\n",
        "        w.writeheader()\n",
        "        for r in kept:\n",
        "            w.writerow(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e5aa16",
      "metadata": {
        "id": "86e5aa16"
      },
      "source": [
        "### 1. Configuración y utilidades iniciales  \n",
        "\n",
        "Importa librerías, define rutas estándar para el proyecto, crea las carpetas necesarias, configura un entorno seguro para ejecutar comandos de EDirect/requests, y declara funciones auxiliares (lectura/escritura de TSV, conversión de tipos, conversión km→grados, ejecución robusta de comandos y filtrado de metadatos WGS+ILLUMINA).  \n",
        "\n",
        "**Entradas:**  \n",
        "Ninguna (solo variables de entorno como `MAGENTA_DIR` si están definidas).  \n",
        "\n",
        "**Salidas:**  \n",
        "- Estructura de carpetas lista (`rawdata`, `metadata`, `outputs`, `logs`, `aux`)  \n",
        "- Variables globales de rutas  \n",
        "- Entorno seguro de red para `requests` y `curl/EDirect`  \n",
        "- Funciones auxiliares disponibles (`save_tsv`, `load_tsv`, `ensure_float`, `km_to_deg`, `run_cmd`, `have_edirect`, `postfilter_wgs_illumina_csv`)  \n",
        "\n",
        "**Parámetros editables:**  \n",
        "- `MAGENTA_DIR`  \n",
        "- `EUTILS_TOOL`  \n",
        "- `EUTILS_EMAIL`  \n",
        "- Número de reintentos en `run_cmd`  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fcee0aa",
      "metadata": {
        "id": "8fcee0aa"
      },
      "source": [
        "# 1) Descarga de metadatos (ENA + NCBI)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e5b75d",
      "metadata": {
        "id": "17e5b75d"
      },
      "source": [
        "### 1.1 ENA Portal API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea62845",
      "metadata": {
        "id": "9ea62845"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "ENA_BASE = \"https://www.ebi.ac.uk/ena/portal/api/search\"\n",
        "\n",
        "def ena_query_for_mangrove() -> str:\n",
        "    # scientific_name preferido + texto libre 'mangrove' como respaldo\n",
        "    terms = '(scientific_name=\"mangrove metagenome\" OR mangrove)'\n",
        "    return f'(library_strategy=\"WGS\" AND instrument_platform=\"ILLUMINA\" AND {terms})'\n",
        "\n",
        "ENA_FIELDS = [\n",
        "    \"run_accession\",\"sample_accession\",\"study_accession\",\n",
        "    \"library_strategy\",\"library_layout\",\"instrument_platform\",\n",
        "    \"collection_date\",\"country\",\n",
        "    \"location\",\"lat\",\"lon\",\n",
        "    \"fastq_ftp\",\"fastq_http\",\"fastq_md5\"\n",
        "]\n",
        "\n",
        "params = {\n",
        "    \"result\": \"read_run\",\n",
        "    \"query\":  ena_query_for_mangrove(),\n",
        "    \"fields\": \",\".join(ENA_FIELDS),\n",
        "    \"format\": \"tsv\",\n",
        "    \"limit\":  0\n",
        "}\n",
        "url = f\"{ENA_BASE}?{urlencode(params)}\"\n",
        "print(\"URL ENA:\", url)\n",
        "\n",
        "r = requests.get(url, timeout=180, verify=CA_BUNDLE)\n",
        "r.raise_for_status()\n",
        "\n",
        "ena_tsv = META_DIR / \"ena_mangrove.tsv\"\n",
        "with open(ena_tsv, \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "ena_df = pd.read_csv(ena_tsv, sep=\"\\t\", dtype=str)\n",
        "print(\"ENA metadatos filas:\", len(ena_df), \"->\", ena_tsv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61005b5f",
      "metadata": {
        "id": "61005b5f"
      },
      "source": [
        "## ENA Portal API — Manglar  \n",
        "\n",
        "Consulta el portal de ENA (API) con una query específica para **metagenomas de manglar**. Luego los carga en un DataFrame de pandas para su posterior análisis.  \n",
        "\n",
        "**Entradas:**  \n",
        "- Endpoint público de ENA (`https://www.ebi.ac.uk/ena/portal/api/search`)  \n",
        "- Parámetros de búsqueda (`params`) incluyendo filtro por:  \n",
        "  - `library_strategy=\"WGS\"`  \n",
        "  - `instrument_platform=\"ILLUMINA\"`  \n",
        "  - `scientific_name=\"mangrove metagenome\"` o término libre `mangrove`.  \n",
        "\n",
        "**Salidas:**  \n",
        "- Archivo `metadata/ena_mangrove.tsv`  \n",
        "- DataFrame `ena_df` en memoria con los metadatos cargados.  \n",
        "\n",
        "**Parámetros editables:**  \n",
        "- `ena_query_for_mangrove()` (criterio de búsqueda en ENA)  \n",
        "- `ENA_FIELDS` (campos a descargar, incluye coordenadas `lat`, `lon` y `location`)  \n",
        "- `limit` (número de resultados, 0 = todos)  \n",
        "- `timeout` (segundos de espera de la petición HTTP)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2113834",
      "metadata": {
        "id": "b2113834"
      },
      "source": [
        "### 1.2 NCBI/SRA — EDirect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05cf39b",
      "metadata": {
        "id": "d05cf39b"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "query_ncbi = (\n",
        "    '(\"WGS\"[Strategy] AND \"ILLUMINA\"[Platform]) AND '\n",
        "    '( ( mangrove[All Fields] OR mangroves[All Fields] ) '\n",
        "    '  AND ( metagenome[All Fields] OR metagenomic[All Fields] OR metagenomics[All Fields] ) )'\n",
        ")\n",
        "\n",
        "sra_raw_csv = META_DIR / \"sra_mangrove_raw_runinfo.csv\"\n",
        "sra_csv     = META_DIR / \"sra_mangrove_wgs_illumina.csv\"\n",
        "\n",
        "cmd = 'esearch -db sra -query {q} | efetch -format runinfo > {out}'.format(\n",
        "    q=shlex.quote(query_ncbi),\n",
        "    out=shlex.quote(str(sra_raw_csv))\n",
        ")\n",
        "print(\"Comando EDirect:\")\n",
        "print(cmd)\n",
        "\n",
        "ret = run_cmd(cmd)\n",
        "need_fallback = (ret != 0) or (not sra_raw_csv.exists()) or (sra_raw_csv.stat().st_size == 0)\n",
        "if need_fallback:\n",
        "    print(\"[SRA][WARN] EDirect falló o sin filas. Intentando fallback RunInfo CSV…\")\n",
        "    url_fb = \"https://trace.ncbi.nlm.nih.gov/Traces/sra-db-be/runinfo?term=\" + quote_plus(query_ncbi)\n",
        "    print(\"[SRA][fallback] GET\", url_fb)\n",
        "    try:\n",
        "        rr = requests.get(url_fb, timeout=180, verify=CA_BUNDLE)\n",
        "        rr.raise_for_status()\n",
        "        txt = rr.text\n",
        "        if txt and \"Run,ReleaseDate,LoadDate\" in txt.splitlines()[0]:\n",
        "            sra_raw_csv.write_text(txt, encoding=\"utf-8\")\n",
        "            print(\"[SRA][fallback] OK ->\", sra_raw_csv)\n",
        "        else:\n",
        "            print(\"[SRA][fallback][WARN] Respuesta no luce como RunInfo CSV.\")\n",
        "    except Exception as e:\n",
        "        print(\"[SRA][fallback][ERROR]\", e)\n",
        "\n",
        "postfilter_wgs_illumina_csv(sra_raw_csv, sra_csv)\n",
        "if sra_csv.exists() and sra_csv.stat().st_size > 0:\n",
        "    print(\"NCBI RunInfo (filtrado WGS+ILLUMINA) ->\", sra_csv)\n",
        "else:\n",
        "    print(\"NCBI RunInfo no disponible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f867f196",
      "metadata": {
        "id": "f867f196"
      },
      "source": [
        "## NCBI/SRA — EDirect con fallback RunInfo (Manglar)  \n",
        "\n",
        "Ejecuta una consulta en **NCBI SRA** usando **EDirect** (`esearch` + `efetch`) para obtener metadatos `RunInfo` de corridas relacionadas con **metagenomas de manglar**.  \n",
        "- Refuerza el filtrado a **WGS + ILLUMINA** mediante un post-procesamiento.  \n",
        "- Si EDirect falla o devuelve vacío, recurre a un **fallback** descargando el `RunInfo CSV` directamente desde el portal web de NCBI.  \n",
        "\n",
        "**Entradas:**  \n",
        "- Herramientas `esearch` y `efetch` disponibles en el `PATH`.  \n",
        "- Endpoint web de NCBI SRA RunInfo como respaldo.  \n",
        "- Query definida en `query_ncbi` que incluye:  \n",
        "  - `\"WGS\"[Strategy] AND \"ILLUMINA\"[Platform]`  \n",
        "  - `(mangrove OR mangroves)`  \n",
        "  - `(metagenome OR metagenomic OR metagenomics)`  \n",
        "\n",
        "**Salidas:**  \n",
        "- `metadata/sra_mangrove_raw_runinfo.csv` → resultado bruto (EDirect o fallback).  \n",
        "- `metadata/sra_mangrove_wgs_illumina.csv` → resultado filtrado solo con corridas WGS+ILLUMINA.  \n",
        "\n",
        "**Parámetros editables:**  \n",
        "- `query_ncbi` (criterio de búsqueda en NCBI).  \n",
        "- Rutas de salida (`sra_raw_csv`, `sra_csv`).  \n",
        "- `timeout` (segundos de espera en el fallback HTTP).  \n",
        "- Número de reintentos en `run_cmd`.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c903a7a3",
      "metadata": {
        "id": "c903a7a3"
      },
      "source": [
        "### 1.3 Unificación y filtro técnico mínimo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938a0703",
      "metadata": {
        "id": "938a0703"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "def normalize_ena(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.rename(columns={\n",
        "        \"run_accession\":      \"Run\",\n",
        "        \"sample_accession\":   \"Sample\",\n",
        "        \"study_accession\":    \"Study\",\n",
        "        \"library_layout\":     \"LibraryLayout\",\n",
        "        \"library_strategy\":   \"LibraryStrategy\",\n",
        "        \"instrument_platform\":\"Platform\",\n",
        "        \"lat\":                \"latitude\",\n",
        "        \"lon\":                \"longitude\",\n",
        "        \"location\":           \"geographic_location\",\n",
        "    })\n",
        "\n",
        "def normalize_sra(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    ren = {}\n",
        "    if \"lat\" in df.columns: ren[\"lat\"] = \"latitude\"\n",
        "    if \"lon\" in df.columns: ren[\"lon\"] = \"longitude\"\n",
        "    return df.rename(columns=ren)\n",
        "\n",
        "def _parse_ns_ew(val: str):\n",
        "    if not isinstance(val, str):\n",
        "        return None, None\n",
        "    tokens = re.findall(r'([+-]?\\d+(?:\\.\\d+)?)([NnSsEeWw]?)', val.strip())\n",
        "    nums = []\n",
        "    for num, hemi in tokens:\n",
        "        x = float(num)\n",
        "        if hemi.upper() == 'S': x = -abs(x)\n",
        "        elif hemi.upper() == 'N': x =  abs(x)\n",
        "        elif hemi.upper() == 'W': x = -abs(x)\n",
        "        elif hemi.upper() == 'E': x =  abs(x)\n",
        "        nums.append(x)\n",
        "    if len(nums) >= 2:\n",
        "        return nums[0], nums[1]\n",
        "    return None, None\n",
        "\n",
        "def _extract_lat_lon_from_text(s: str):\n",
        "    if not isinstance(s, str):\n",
        "        return np.nan, np.nan\n",
        "    lat, lon = _parse_ns_ew(s)\n",
        "    if lat is not None and lon is not None:\n",
        "        return lat, lon\n",
        "    nums = re.findall(r'([+-]?\\d+(?:\\.\\d+)?)', s)\n",
        "    if len(nums) >= 2:\n",
        "        return float(nums[0]), float(nums[1])\n",
        "    return np.nan, np.nan\n",
        "\n",
        "def _valid_lat_lon(lat, lon):\n",
        "    try:\n",
        "        if pd.isna(lat) or pd.isna(lon):\n",
        "            return False\n",
        "        lat = float(lat); lon = float(lon)\n",
        "        return np.isfinite(lat) and np.isfinite(lon) and -90.0 <= lat <= 90.0 and -180.0 <= lon <= 180.0\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "ENA_BASE = \"https://www.ebi.ac.uk/ena/portal/api/search\"\n",
        "\n",
        "def enrich_coords_from_ena_by_runs(run_ids, chunk_size=200):\n",
        "    \"\"\"Devuelve DataFrame: Run, latitude, longitude, geographic_location (ENA read_run).\"\"\"\n",
        "    records = []\n",
        "    run_ids = [r for r in run_ids if isinstance(r, str) and r.strip()]\n",
        "    for i in range(0, len(run_ids), chunk_size):\n",
        "        chunk = run_ids[i:i+chunk_size]\n",
        "        ors = \" OR \".join([f'run_accession=\"{r}\"' for r in chunk])\n",
        "        params = {\n",
        "            \"result\": \"read_run\",\n",
        "            \"query\":  ors,\n",
        "            \"fields\": \",\".join([\"run_accession\", \"lat\", \"lon\", \"location\"]),\n",
        "            \"format\": \"tsv\",\n",
        "            \"limit\":  0,\n",
        "        }\n",
        "        url = f\"{ENA_BASE}?{urlencode(params)}\"\n",
        "        try:\n",
        "            r = requests.get(url, timeout=180, verify=CA_BUNDLE)\n",
        "            r.raise_for_status()\n",
        "            if r.content:\n",
        "                dfc = pd.read_csv(io.StringIO(r.content.decode(\"utf-8\")), sep=\"\\t\", dtype=str)\n",
        "                if not dfc.empty:\n",
        "                    dfc = dfc.rename(columns={\n",
        "                        \"run_accession\": \"Run\",\n",
        "                        \"lat\": \"latitude\",\n",
        "                        \"lon\": \"longitude\",\n",
        "                        \"location\": \"geographic_location\",\n",
        "                    })\n",
        "                    records.append(dfc)\n",
        "        except Exception as e:\n",
        "            print(f\"[ENA][enrich run][WARN] chunk {i//chunk_size}: {e}\")\n",
        "    if not records:\n",
        "        return pd.DataFrame(columns=[\"Run\",\"latitude\",\"longitude\",\"geographic_location\"])\n",
        "    return pd.concat(records, ignore_index=True, sort=False)\n",
        "\n",
        "def enrich_coords_from_ena_by_samples(sample_ids, chunk_size=200):\n",
        "    \"\"\"Devuelve DataFrame: Sample, latitude, longitude, geographic_location (ENA read_sample).\"\"\"\n",
        "    records = []\n",
        "    sample_ids = [s for s in sample_ids if isinstance(s, str) and s.strip()]\n",
        "    for i in range(0, len(sample_ids), chunk_size):\n",
        "        chunk = sample_ids[i:i+chunk_size]\n",
        "        ors = \" OR \".join([f'sample_accession=\"{s}\"' for s in chunk])\n",
        "        params = {\n",
        "            \"result\": \"sample\",\n",
        "            \"query\":  ors,\n",
        "            \"fields\": \",\".join([\"sample_accession\",\"lat\",\"lon\",\"location\",\"country\"]),\n",
        "            \"format\": \"tsv\",\n",
        "            \"limit\":  0,\n",
        "        }\n",
        "        url = f\"{ENA_BASE}?{urlencode(params)}\"\n",
        "        try:\n",
        "            r = requests.get(url, timeout=180, verify=CA_BUNDLE)\n",
        "            r.raise_for_status()\n",
        "            if r.content:\n",
        "                dfc = pd.read_csv(io.StringIO(r.content.decode(\"utf-8\")), sep=\"\\t\", dtype=str)\n",
        "                if not dfc.empty:\n",
        "                    dfc = dfc.rename(columns={\n",
        "                        \"sample_accession\": \"Sample\",\n",
        "                        \"lat\": \"latitude\",\n",
        "                        \"lon\": \"longitude\",\n",
        "                        \"location\": \"geographic_location\",\n",
        "                    })\n",
        "                    records.append(dfc)\n",
        "        except Exception as e:\n",
        "            print(f\"[ENA][enrich sample][WARN] chunk {i//chunk_size}: {e}\")\n",
        "    if not records:\n",
        "        return pd.DataFrame(columns=[\"Sample\",\"latitude\",\"longitude\",\"geographic_location\"])\n",
        "    return pd.concat(records, ignore_index=True, sort=False)\n",
        "\n",
        "def fill_and_filter_geo(all_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Asegura columnas mínimas\n",
        "    for col in [\"latitude\",\"longitude\",\"geographic_location\",\"Sample\",\"Run\"]:\n",
        "        if col not in all_df.columns:\n",
        "            all_df[col] = np.nan\n",
        "\n",
        "    # (1) ENA por Run\n",
        "    need_geo = all_df[\"Run\"].notna() & (all_df[\"latitude\"].isna() | all_df[\"longitude\"].isna())\n",
        "    runs_to_enrich = all_df.loc[need_geo, \"Run\"].dropna().astype(str).unique().tolist()\n",
        "    if runs_to_enrich:\n",
        "        print(f\"[GEO] ENA read_run por Run (n={len(runs_to_enrich)})…\")\n",
        "        enr = enrich_coords_from_ena_by_runs(runs_to_enrich, chunk_size=200)\n",
        "        if not enr.empty:\n",
        "            all_df = all_df.merge(enr[[\"Run\",\"latitude\",\"longitude\",\"geographic_location\"]],\n",
        "                                  on=\"Run\", how=\"left\", suffixes=(\"\", \"_ena\"))\n",
        "            for col in [\"latitude\",\"longitude\",\"geographic_location\"]:\n",
        "                all_df[col] = all_df[col].fillna(all_df.get(f\"{col}_ena\"))\n",
        "                if f\"{col}_ena\" in all_df.columns:\n",
        "                    all_df.drop(columns=[f\"{col}_ena\"], inplace=True)\n",
        "\n",
        "    # (2) ENA por Sample\n",
        "    still_missing = (all_df[\"latitude\"].isna() | all_df[\"longitude\"].isna()) & all_df[\"Sample\"].notna()\n",
        "    samples_to_enrich = all_df.loc[still_missing, \"Sample\"].dropna().astype(str).unique().tolist()\n",
        "    if samples_to_enrich:\n",
        "        print(f\"[GEO] ENA sample por Sample (n={len(samples_to_enrich)})…\")\n",
        "        ens = enrich_coords_from_ena_by_samples(samples_to_enrich, chunk_size=200)\n",
        "        if not ens.empty:\n",
        "            all_df = all_df.merge(ens[[\"Sample\",\"latitude\",\"longitude\",\"geographic_location\"]],\n",
        "                                  on=\"Sample\", how=\"left\", suffixes=(\"\", \"_sena\"))\n",
        "            for col in [\"latitude\",\"longitude\",\"geographic_location\"]:\n",
        "                all_df[col] = all_df[col].fillna(all_df.get(f\"{col}_sena\"))\n",
        "                if f\"{col}_sena\" in all_df.columns:\n",
        "                    all_df.drop(columns=[f\"{col}_sena\"], inplace=True)\n",
        "\n",
        "    # (3) Parseo texto geographic_location\n",
        "    still_missing2 = all_df[\"latitude\"].isna() | all_df[\"longitude\"].isna()\n",
        "    if still_missing2.any():\n",
        "        print(f\"[GEO] Parseando geographic_location para {int(still_missing2.sum())} filas…\")\n",
        "        latlon = all_df.loc[still_missing2, \"geographic_location\"].apply(_extract_lat_lon_from_text)\n",
        "        lat_parsed = latlon.map(lambda t: t[0])\n",
        "        lon_parsed = latlon.map(lambda t: t[1])\n",
        "        all_df.loc[still_missing2, \"latitude\"]  = all_df.loc[still_missing2, \"latitude\"].fillna(lat_parsed)\n",
        "        all_df.loc[still_missing2, \"longitude\"] = all_df.loc[still_missing2, \"longitude\"].fillna(lon_parsed)\n",
        "\n",
        "    # Convertir a numérico y validar\n",
        "    all_df[\"latitude\"]  = pd.to_numeric(all_df[\"latitude\"], errors=\"coerce\")\n",
        "    all_df[\"longitude\"] = pd.to_numeric(all_df[\"longitude\"], errors=\"coerce\")\n",
        "    valid_mask = all_df.apply(lambda r: _valid_lat_lon(r[\"latitude\"], r[\"longitude\"]), axis=1)\n",
        "    before_n = len(all_df)\n",
        "    all_df = all_df[valid_mask].copy()\n",
        "    after_n  = len(all_df)\n",
        "    print(f\"[GEO] Filtrado por coordenadas válidas: {before_n} -> {after_n}\")\n",
        "    return all_df\n",
        "\n",
        "# ---- Unificación ----\n",
        "ena_df_norm = pd.DataFrame()\n",
        "sra_df_norm = pd.DataFrame()\n",
        "\n",
        "if 'ena_df' in globals() and not ena_df.empty:\n",
        "    ena_df_norm = normalize_ena(ena_df.copy())\n",
        "    ena_df_norm[\"source\"] = \"ENA\"\n",
        "    ena_df_norm[\"ecosystem\"] = \"mangrove\"\n",
        "\n",
        "if (META_DIR / \"sra_mangrove_wgs_illumina.csv\").exists():\n",
        "    sra_df = pd.read_csv(META_DIR / \"sra_mangrove_wgs_illumina.csv\", dtype=str)\n",
        "    if not sra_df.empty:\n",
        "        sra_df_norm = normalize_sra(sra_df.copy())\n",
        "        sra_df_norm[\"source\"] = \"SRA\"\n",
        "        sra_df_norm[\"ecosystem\"] = \"mangrove\"\n",
        "\n",
        "frames = [x for x in [ena_df_norm, sra_df_norm] if not x.empty]\n",
        "if not frames:\n",
        "    print(\"[WARN] No hay tablas para unir.\")\n",
        "    all_df = pd.DataFrame()\n",
        "else:\n",
        "    all_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "\n",
        "# Columnas mínimas\n",
        "for col in [\"Run\",\"Sample\",\"Study\",\"LibraryLayout\",\"LibraryStrategy\",\"Platform\",\n",
        "            \"latitude\",\"longitude\",\"geographic_location\",\"ecosystem\",\"source\"]:\n",
        "    if col not in all_df.columns:\n",
        "        all_df[col] = np.nan\n",
        "\n",
        "# Filtro WGS + ILLUMINA y de-dup por Run\n",
        "mask = (\n",
        "    all_df[\"LibraryStrategy\"].fillna(\"\").str.upper().str.contains(\"WGS\") &\n",
        "    all_df[\"Platform\"].fillna(\"\").str.upper().str.contains(\"ILLUMINA\")\n",
        ")\n",
        "all_df = all_df[mask].copy()\n",
        "all_df[\"Run\"] = all_df[\"Run\"].astype(str)\n",
        "all_df = all_df.drop_duplicates(subset=[\"Run\"], keep=\"first\")\n",
        "\n",
        "# Enriquecer coordenadas y filtrar geo válido\n",
        "if not all_df.empty:\n",
        "    all_df = fill_and_filter_geo(all_df)\n",
        "\n",
        "# Exportar\n",
        "if all_df.empty:\n",
        "    print(\"[WARN] Tras filtrar por coordenadas válidas no quedaron corridas.\")\n",
        "    out_eco = OUT_DIR / \"mangrove_metadata.tsv\"\n",
        "    save_tsv(pd.DataFrame(), out_eco)\n",
        "    unified_tsv = META_DIR / \"metadatos_unificados.tsv\"\n",
        "    save_tsv(pd.DataFrame(), unified_tsv)\n",
        "else:\n",
        "    out_eco = OUT_DIR / \"mangrove_metadata.tsv\"\n",
        "    save_tsv(all_df, out_eco)\n",
        "    unified_tsv = META_DIR / \"metadatos_unificados.tsv\"\n",
        "    save_tsv(all_df, unified_tsv)\n",
        "    print(f\"[OK] mangrove: {len(all_df)} filas -> {out_eco}\")\n",
        "    print(f\"[OK] Metadatos unificados (manglar, geo-válidos) -> {unified_tsv}\")\n",
        "\n",
        "    summary = (\n",
        "        all_df.groupby([\"ecosystem\",\"source\"])[\"Run\"]\n",
        "        .nunique()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"Run\":\"n_runs\"})\n",
        "        .sort_values([\"ecosystem\",\"source\"])\n",
        "    )\n",
        "    log_path = LOGS_DIR / \"summary_mangrove_wgs_illumina.tsv\"\n",
        "    save_tsv(summary, log_path)\n",
        "    print(f\"[OK] Resumen -> {log_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca0b965e",
      "metadata": {
        "id": "ca0b965e"
      },
      "source": [
        "# Unificación y filtros técnicos de metadatos genómicos\n",
        "\n",
        "Combina y depura metadatos de **ENA** y **NCBI/SRA** para generar un dataset unificado con coordenadas geográficas validadas.\n",
        "\n",
        "1. **Normalización** de columnas (`normalize_ena`, `normalize_sra`).\n",
        "2. **Unificación** de tablas de ENA y SRA.\n",
        "3. **Filtrado técnico**:  \n",
        "   - Mantiene solo registros con `LibraryStrategy` metagenómica.  \n",
        "   - (Opcional) restringe además a `WGS` y `ILLUMINA`.  \n",
        "4. **Eliminación de duplicados** por `Run`.\n",
        "5. **Enriquecimiento geográfico**:  \n",
        "   - Consulta coordenadas en ENA (`read_run`, `sample`).  \n",
        "   - Parseo de campos de texto (`geographic_location`).  \n",
        "   - Validación de lat/lon.\n",
        "6. **Exportación** de resultados en formatos `TSV`.\n",
        "\n",
        "---\n",
        "\n",
        "##  Funciones principales\n",
        "\n",
        "- `normalize_ena(df)`: renombra columnas clave de ENA.  \n",
        "- `normalize_sra(df)`: renombra columnas clave de SRA.  \n",
        "- `_parse_ns_ew(val)`: interpreta coordenadas con hemisferio N/S/E/W.  \n",
        "- `_extract_lat_lon_from_text(s)`: extrae lat/lon desde cadenas libres.  \n",
        "- `_valid_lat_lon(lat, lon)`: valida rango y formato de coordenadas.  \n",
        "- `enrich_coords_from_ena_by_runs(run_ids)`: obtiene coordenadas desde ENA por *Run*.  \n",
        "- `enrich_coords_from_ena_by_samples(sample_ids)`: obtiene coordenadas desde ENA por *Sample*.  \n",
        "- `fill_and_filter_geo(all_df)`: completa coordenadas faltantes y filtra inválidas.\n",
        "\n",
        "---\n",
        "\n",
        "##  Entradas\n",
        "\n",
        "- `metadata/ena_mangrove.tsv` → tabla ENA.  \n",
        "- `metadata/sra_mangrove_wgs_illumina.csv` → tabla NCBI/SRA.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Salidas\n",
        "\n",
        "- `metadata/metadatos_unificados.tsv` → tabla consolidada.  \n",
        "- `output/mangrove_metadata.tsv` → dataset final filtrado por ecosistema.  \n",
        "- `logs/summary_mangrove_wgs_illumina.tsv` → resumen por `ecosystem` y `source`.  \n",
        "- `metadata/metadatos_unificados.csv` → versión CSV con todos los metadatos asociados.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Parámetros editables\n",
        "\n",
        "- **Mapeos de columnas** (`rename`).  \n",
        "- **Filtros aplicados**:  \n",
        "  - `metagenomic`  \n",
        "  - `WGS`  \n",
        "  - `ILLUMINA`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfc23e32",
      "metadata": {
        "id": "dfc23e32"
      },
      "source": [
        "## 2) Descarga y conversión de lecturas (ENA/NCBI)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c18cc47e",
      "metadata": {
        "id": "c18cc47e"
      },
      "source": [
        "### 2.1 ENA — descarga paralela con aria2c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c045d0",
      "metadata": {
        "id": "44c045d0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import subprocess\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "from urllib.parse import unquote\n",
        "from shutil import which\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Rutas fijas del proyecto (ajústalas si hace falta)\n",
        "# ---------------------------------------------------------------------\n",
        "PROJECT_DIR = Path(\"/nfs/testing/.jbalvino/MAGENTA/MAGENTA_DIR\").resolve()\n",
        "META_PATH   = PROJECT_DIR / \"metadata\" / \"metadatos_unificados.tsv\"\n",
        "ENA_DIR     = PROJECT_DIR / \"rawdata\" / \"ena\"\n",
        "CONV_DIR    = PROJECT_DIR / \"rawdata\" / \"convertidos\"\n",
        "TMP_FQD     = PROJECT_DIR / \"tmp_fqd\"\n",
        "LOGS_DIR    = PROJECT_DIR / \"logs\"\n",
        "AUX_DIR     = PROJECT_DIR / \"aux\"\n",
        "SRA_IMG     = \"docker://ncbi/sra-tools:3.1.1\"  # fallback en contenedor (si existe)\n",
        "\n",
        "for d in (ENA_DIR, CONV_DIR, TMP_FQD, LOGS_DIR, AUX_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not META_PATH.exists():\n",
        "    print(f\"[ERROR] No se encuentra {META_PATH}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "RUNLOG = (LOGS_DIR / f\"descarga_all_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log\").open(\"w\")\n",
        "\n",
        "def logprint(*args):\n",
        "    msg = \" \".join(str(a) for a in args)\n",
        "    print(msg)\n",
        "    try:\n",
        "        RUNLOG.write(msg + \"\\n\"); RUNLOG.flush()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def run_cmd(cmd_list, check=True, env=None):\n",
        "    \"\"\"Ejecuta comando mostrando stdout/stderr en el log. Devuelve True/False.\"\"\"\n",
        "    logprint(\"[CMD]\", \" \".join(cmd_list))\n",
        "    try:\n",
        "        res = subprocess.run(\n",
        "            cmd_list, check=check, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env\n",
        "        )\n",
        "        out = (res.stdout.decode(errors=\"ignore\") + res.stderr.decode(errors=\"ignore\")).strip()\n",
        "        if out:\n",
        "            logprint(\"[OUT]\", out)\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        out = ((e.stdout.decode(errors=\"ignore\") if e.stdout else \"\") +\n",
        "               (e.stderr.decode(errors=\"ignore\") if e.stderr else \"\")).strip()\n",
        "        if out:\n",
        "            logprint(\"[ERR]\", out)\n",
        "        return False\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# EXCLUSIONES (opcional)\n",
        "# ---------------------------------------------------------------------\n",
        "EXCLUDE_RUNS = set(r.strip() for r in os.environ.get(\"EXCLUDE_RUNS\", \"\").split(\",\") if r.strip())\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# CA corporativa / TLS\n",
        "# ---------------------------------------------------------------------\n",
        "def apply_corporate_ca():\n",
        "    corp_ca = os.environ.get(\"CORP_CA\", \"\").strip()\n",
        "    sys_cas = [\n",
        "        \"/etc/pki/tls/certs/ca-bundle.crt\",\n",
        "        \"/etc/ssl/certs/ca-certificates.crt\",\n",
        "        \"/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\",\n",
        "    ]\n",
        "    vdbc = which(\"vdb-config\")\n",
        "    def vdb_set(k, v):\n",
        "        if vdbc:\n",
        "            run_cmd([vdbc, \"-s\", f\"{k}={v}\"], check=False)\n",
        "\n",
        "    if vdbc:\n",
        "        run_cmd([vdbc, \"--restore-defaults\"], check=False)\n",
        "\n",
        "    if corp_ca and Path(corp_ca).exists():\n",
        "        os.environ[\"SSL_CERT_FILE\"] = corp_ca\n",
        "        os.environ[\"REQUESTS_CA_BUNDLE\"] = corp_ca\n",
        "        os.environ[\"CURL_CA_BUNDLE\"] = corp_ca\n",
        "        vdb_set(\"/tls/verify-peer\", \"true\")\n",
        "        vdb_set(\"/tls/use-system-ca-cert\", \"false\")\n",
        "        vdb_set(\"/tls/ca-file\", corp_ca)\n",
        "        logprint(f\"[TLS] CA corporativa aplicada: {corp_ca}\")\n",
        "        return corp_ca\n",
        "\n",
        "    for p in sys_cas:\n",
        "        if Path(p).exists():\n",
        "            os.environ.setdefault(\"SSL_CERT_FILE\", p)\n",
        "            os.environ.setdefault(\"REQUESTS_CA_BUNDLE\", p)\n",
        "            os.environ.setdefault(\"CURL_CA_BUNDLE\", p)\n",
        "            vdb_set(\"/tls/verify-peer\", \"true\")\n",
        "            vdb_set(\"/tls/use-system-ca-cert\", \"yes\")\n",
        "            vdb_set(\"/tls/ca-file\", p)\n",
        "            logprint(f\"[TLS] CA del sistema aplicada: {p}\")\n",
        "            return p\n",
        "\n",
        "    vdb_set(\"/tls/verify-peer\", \"true\")\n",
        "    vdb_set(\"/tls/use-system-ca-cert\", \"yes\")\n",
        "    logprint(\"[TLS] Sin CA explícita; configuración por defecto.\")\n",
        "    return None\n",
        "\n",
        "CA_IN_USE = apply_corporate_ca()\n",
        "\n",
        "def wget_base_args():\n",
        "    base = [which(\"wget\") or \"wget\", \"-c\", \"--tries=10\", \"--read-timeout=30\"]\n",
        "    if os.environ.get(\"ALLOW_INSECURE_WGET\") == \"1\":\n",
        "        base.insert(1, \"--no-check-certificate\")\n",
        "    elif CA_IN_USE and Path(CA_IN_USE).exists():\n",
        "        base.extend([\"--ca-certificate\", CA_IN_USE])\n",
        "    return base\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Utilidades generales\n",
        "# ---------------------------------------------------------------------\n",
        "def sra_tools_version():\n",
        "    fqd = which(\"fasterq-dump\")\n",
        "    if not fqd:\n",
        "        return (0, 0, 0)\n",
        "    ok = subprocess.run([fqd, \"--version\"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
        "    m = re.search(r\"fasterq-dump\\.(\\d+)\\.(\\d+)\\.(\\d+)\", ok.stdout.decode())\n",
        "    if m:\n",
        "        return tuple(int(x) for x in m.groups())\n",
        "    return (0, 0, 0)\n",
        "\n",
        "SRA_VER = sra_tools_version()\n",
        "logprint(f\"[SRA-TOOLS] fasterq-dump versión detectada: {'.'.join(map(str,SRA_VER))}\")\n",
        "\n",
        "def parse_semicolon_urls(s: str):\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    return [u.strip() for u in s.split(\";\") if u and u.strip()]\n",
        "\n",
        "def list_fastq_for_run(base_dir: Path, run_id: str):\n",
        "    outs = []\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        for f in files:\n",
        "            if f.endswith((\".fastq\", \".fastq.gz\")) and (f.startswith(run_id) or f.startswith(f\"{run_id}.sra\")):\n",
        "                outs.append(Path(root) / f)\n",
        "    return sorted(outs)\n",
        "\n",
        "def have_apptainer():\n",
        "    return bool(which(\"apptainer\") or which(\"singularity\"))\n",
        "\n",
        "def digits_after_prefix(run_id: str) -> str:\n",
        "    # p.ej. SRR29002871 -> \"29002871\"\n",
        "    return re.sub(r\"^[A-Za-z]+\", \"\", run_id)\n",
        "\n",
        "def ena_dir2(run_id: str) -> str | None:\n",
        "    # Regla ENA: 7 dígitos -> \"00x\"; 8 -> \"0xx\"; 9+ -> \"xxx\"; <=6 -> sin subcarpeta\n",
        "    d = digits_after_prefix(run_id)\n",
        "    L = len(d)\n",
        "    if L <= 6: return None\n",
        "    if L == 7:  return f\"00{d[-1]}\"\n",
        "    if L == 8:  return f\"0{d[-2:]}\"\n",
        "    return d[-3:]  # 9+\n",
        "\n",
        "def ena_fastq_candidates(run_id: str):\n",
        "    pre6 = run_id[:6]\n",
        "    d2 = ena_dir2(run_id)\n",
        "    parts = []\n",
        "    if d2:\n",
        "        parts.append((pre6, d2, run_id))\n",
        "    parts.append((pre6, run_id))  # fallback sin d2\n",
        "    urls = []\n",
        "    for scheme in (\"https\", \"http\", \"ftp\"):\n",
        "        for p in parts:\n",
        "            base = f\"{scheme}://ftp.sra.ebi.ac.uk/vol1/fastq/\" + \"/\".join(p)\n",
        "            urls += [f\"{base}/{run_id}_1.fastq.gz\", f\"{base}/{run_id}_2.fastq.gz\"]\n",
        "    return urls\n",
        "\n",
        "def ena_sra_candidates(run_id: str):\n",
        "    pre6 = run_id[:6]\n",
        "    d2 = ena_dir2(run_id)\n",
        "    tuples = []\n",
        "    if d2:\n",
        "        tuples.append((pre6, d2, run_id))\n",
        "    tuples.append((pre6, run_id))\n",
        "    urls = []\n",
        "    for scheme in (\"https\", \"http\", \"ftp\"):\n",
        "        for p in tuples:\n",
        "            base = f\"{scheme}://ftp.sra.ebi.ac.uk/vol1/srr/\" + \"/\".join(p)\n",
        "            urls.append(f\"{base}/{run_id}.sra\")\n",
        "    return urls\n",
        "\n",
        "def wget_spider(url: str) -> bool:\n",
        "    cmd = wget_base_args() + [\"--spider\", url]\n",
        "    ok = run_cmd(cmd, check=False)\n",
        "    return ok\n",
        "\n",
        "def download_from_url(url: str, dest: Path) -> bool:\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if dest.exists() and dest.stat().st_size > 0:\n",
        "        logprint(f\"[DL] Ya existe: {dest.name}\")\n",
        "        return True\n",
        "    cmd = wget_base_args() + [url, \"-O\", str(dest)]\n",
        "    ok = run_cmd(cmd, check=False)\n",
        "    return ok and dest.exists() and dest.stat().st_size > 0\n",
        "\n",
        "def normalize_fastq_names(run_id: str):\n",
        "    \"\"\"Ajusta nombres post-dump: SRR.fastq -> SRR_1.fastq; gestiona single-end y opcional _2 vacío.\"\"\"\n",
        "    outs = list_fastq_for_run(CONV_DIR, run_id)\n",
        "    if not outs:\n",
        "        return\n",
        "\n",
        "    # Caso común con input \"archivo.sra\": queda \"SRRxxxxxx.sra.fastq\"\n",
        "    for p in list(outs):\n",
        "        if p.name.endswith(\".sra.fastq\"):\n",
        "            newp = p.with_name(p.name.replace(\".sra.fastq\", \".fastq\"))\n",
        "            p.rename(newp)\n",
        "            logprint(f\"[FIX] Renombrado {p.name} -> {newp.name}\")\n",
        "    outs = list_fastq_for_run(CONV_DIR, run_id)\n",
        "\n",
        "    # Si hay exactamente 1 archivo y no es *_1/_2, renombrar a _1\n",
        "    only = [p for p in outs if p.name == f\"{run_id}.fastq\" or p.name == f\"{run_id}.fastq.gz\"]\n",
        "    if len(outs) == 1 and only:\n",
        "        p = outs[0]\n",
        "        suf = (p.suffixes[-2] + p.suffixes[-1]) if \"\".join(p.suffixes).endswith(\".fastq.gz\") else p.suffix\n",
        "        target = p.with_name(f\"{run_id}_1{suf}\")\n",
        "        p.rename(target)\n",
        "        logprint(f\"[FIX] Dataset single-end: {p.name} -> {target.name}\")\n",
        "        if os.environ.get(\"TOUCH_EMPTY_R2\") == \"1\":\n",
        "            r2 = target.with_name(f\"{run_id}_2{suf}\")\n",
        "            if r2.suffix == \".gz\":\n",
        "                run_cmd([\"bash\", \"-lc\", f\": | gzip -c > {r2}\"], check=False)\n",
        "            else:\n",
        "                r2.touch()\n",
        "            logprint(f\"[FIX] Creado _2 vacío: {r2.name}\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Carga metadatos y detección de columnas\n",
        "# ---------------------------------------------------------------------\n",
        "logprint(f\"[INFO] Cargando {META_PATH}\")\n",
        "df = pd.read_csv(META_PATH, sep=\"\\t\", dtype=str, low_memory=False).fillna(\"\")\n",
        "logprint(f\"[INFO] Filas: {len(df):,}\")\n",
        "\n",
        "cols_lower = {c.lower(): c for c in df.columns}\n",
        "COL_RUN = cols_lower.get(\"run\") or cols_lower.get(\"run_accession\") or cols_lower.get(\"run id\") or cols_lower.get(\"run_id\")\n",
        "\n",
        "LINK_KEYS = (\"ftp_link\", \"download_path\", \"fastq_ftp\", \"fastq_http\", \"sra_link\", \"sra_ftp\", \"sra_http\")\n",
        "COL_LINKS = [cols_lower[k] for k in LINK_KEYS if k in cols_lower]\n",
        "\n",
        "if not COL_RUN:\n",
        "    logprint(\"[ERROR] No se encontró columna Run/run_accession en el TSV.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "def row_get_links(row):\n",
        "    links = []\n",
        "    for col in (COL_LINKS or []):\n",
        "        links += parse_semicolon_urls(str(row.get(col, \"\")))\n",
        "    if not links:\n",
        "        big = \" \".join(map(str, row.values))\n",
        "        if \";\" in big:\n",
        "            links = parse_semicolon_urls(big)\n",
        "    return links\n",
        "\n",
        "def is_ena_row(row):\n",
        "    return any(\".fastq.gz\" in u.lower() for u in row_get_links(row))\n",
        "\n",
        "def is_ncbi_row(row):\n",
        "    L = [u.lower() for u in row_get_links(row)]\n",
        "    return any(\".lite.1\" in u or u.endswith(\".sra\") for u in L)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ENA directa (si hay)\n",
        "# ---------------------------------------------------------------------\n",
        "def download_ena_row(row):\n",
        "    run = str(row.get(COL_RUN, \"\")).strip()\n",
        "    urls = [u for u in row_get_links(row) if \".fastq.gz\" in u.lower()]\n",
        "    if not urls:\n",
        "        urls = ena_fastq_candidates(run)\n",
        "\n",
        "    got = 0\n",
        "    seen = set()\n",
        "    aria2 = which(\"aria2c\")\n",
        "    if aria2 and urls:\n",
        "        lst = AUX_DIR / f\"ena_{run}.txt\"\n",
        "        with open(lst, \"w\") as f:\n",
        "            for u in urls:\n",
        "                if u not in seen:\n",
        "                    f.write(u + \"\\n\"); seen.add(u)\n",
        "        cmd = [aria2, \"--continue=true\",\n",
        "               \"--max-concurrent-downloads=4\", \"--max-connection-per-server=8\",\n",
        "               \"--retry-wait=5\", \"--max-tries=10\", \"--file-allocation=none\",\n",
        "               f\"--input-file={str(lst)}\", f\"--dir={str(ENA_DIR)}\"]\n",
        "        run_cmd(cmd, check=False)\n",
        "        for p in ENA_DIR.glob(f\"{run}*.fastq.gz\"):\n",
        "            logprint(f\"[ENA] descargado: {p.name}\"); got += 1\n",
        "    else:\n",
        "        for u in urls:\n",
        "            if u in seen: continue\n",
        "            seen.add(u)\n",
        "            fname = ENA_DIR / os.path.basename(unquote(u))\n",
        "            if wget_spider(u) and download_from_url(u, fname):\n",
        "                logprint(f\"[ENA] descargado: {fname.name}\")\n",
        "                got += 1\n",
        "    return got\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Cadena NCBI (igual que el original corregido)\n",
        "# ---------------------------------------------------------------------\n",
        "def fasterq_dump_remote(run_id: str) -> bool:\n",
        "    fqd = which(\"fasterq-dump\")\n",
        "    if not fqd:\n",
        "        return False\n",
        "    threads = os.environ.get(\"THREADS\", \"8\")\n",
        "    cmd = [fqd, run_id, \"--split-files\", \"-O\", str(CONV_DIR),\n",
        "           \"--threads\", threads, \"--temp\", str(TMP_FQD)]\n",
        "    ok = run_cmd(cmd, check=False, env=os.environ.copy())\n",
        "    normalize_fastq_names(run_id)\n",
        "    outs = list_fastq_for_run(CONV_DIR, run_id)\n",
        "    return bool(outs)\n",
        "\n",
        "def fasterq_dump_local(sra_path: Path, run_id: str) -> bool:\n",
        "    fqd = which(\"fasterq-dump\")\n",
        "    if not fqd:\n",
        "        return False\n",
        "    threads = os.environ.get(\"THREADS\", \"8\")\n",
        "    cmd = [fqd, str(sra_path), \"--outdir\", str(CONV_DIR),\n",
        "           \"--split-files\", \"--threads\", threads, \"--temp\", str(TMP_FQD)]\n",
        "    ok = run_cmd(cmd, check=False)\n",
        "    normalize_fastq_names(run_id)\n",
        "    outs = list_fastq_for_run(CONV_DIR, run_id)\n",
        "    if outs and os.environ.get(\"KEEP_SRA\", \"0\") != \"1\":\n",
        "        try: sra_path.unlink(); logprint(f\"[NCBI] Eliminado {sra_path.name}\")\n",
        "        except Exception: pass\n",
        "    return bool(outs)\n",
        "\n",
        "def fastq_dump_local(sra_path: Path, run_id: str) -> bool:\n",
        "    fd = which(\"fastq-dump\")\n",
        "    if not fd:\n",
        "        return False\n",
        "    cmd = [fd, str(sra_path), \"--outdir\", str(CONV_DIR),\n",
        "           \"--split-files\", \"--gzip\", \"--skip-technical\", \"--readids\", \"--dumpbase\", \"--clip\"]\n",
        "    ok = run_cmd(cmd, check=False)\n",
        "    normalize_fastq_names(run_id)\n",
        "    outs = list_fastq_for_run(CONV_DIR, run_id)\n",
        "    if outs and os.environ.get(\"KEEP_SRA\", \"0\") != \"1\":\n",
        "        try: sra_path.unlink(); logprint(f\"[NCBI] Eliminado {sra_path.name}\")\n",
        "        except Exception: pass\n",
        "    return bool(outs)\n",
        "\n",
        "def fasterq_dump_container(sra_path: Path, run_id: str) -> bool:\n",
        "    runner = which(\"apptainer\") or which(\"singularity\")\n",
        "    if not runner:\n",
        "        return False\n",
        "    bind_arg = f\"{str(PROJECT_DIR)}:/work\"\n",
        "    in_path  = f\"/work/{sra_path.relative_to(PROJECT_DIR)}\"\n",
        "    out_path = f\"/work/{CONV_DIR.relative_to(PROJECT_DIR)}\"\n",
        "    tmp_path = f\"/work/{TMP_FQD.relative_to(PROJECT_DIR)}\"\n",
        "    threads = os.environ.get(\"THREADS\", \"8\")\n",
        "    cmd = [runner, \"exec\", \"--cleanenv\", \"--bind\", bind_arg,\n",
        "           SRA_IMG, \"fasterq-dump\", in_path,\n",
        "           \"--outdir\", out_path, \"--split-files\", \"--threads\", threads, \"--temp\", tmp_path]\n",
        "    ok = run_cmd(cmd, check=False)\n",
        "    normalize_fastq_names(run_id)\n",
        "    outs = list_fastq_for_run(CONV_DIR, run_id)\n",
        "    if outs and os.environ.get(\"KEEP_SRA\", \"0\") != \"1\":\n",
        "        try: sra_path.unlink(); logprint(f\"[NCBI] Eliminado {sra_path.name}\")\n",
        "        except Exception: pass\n",
        "    return bool(outs)\n",
        "\n",
        "def download_sra_from_ena(run_id: str) -> Path | None:\n",
        "    for u in ena_sra_candidates(run_id):\n",
        "        if wget_spider(u):\n",
        "            dest = CONV_DIR / f\"{run_id}.sra\"\n",
        "            if download_from_url(u, dest):\n",
        "                logprint(f\"[ENA-SRA] descargado: {dest.name}\")\n",
        "                return dest\n",
        "    logprint(\"[ENA-SRA] No se encontró .sra en ENA.\")\n",
        "    return None\n",
        "\n",
        "def download_fastq_from_ena(run_id: str) -> int:\n",
        "    got = 0\n",
        "    for u in ena_fastq_candidates(run_id):\n",
        "        dest = CONV_DIR / os.path.basename(unquote(u))\n",
        "        if wget_spider(u) and download_from_url(u, dest):\n",
        "            logprint(f\"[ENA-FASTQ] descargado: {dest.name}\")\n",
        "            got += 1\n",
        "    return got\n",
        "\n",
        "def download_ncbi_row_and_convert(row):\n",
        "    run = str(row.get(COL_RUN, \"\")).strip()\n",
        "    links = row_get_links(row)\n",
        "\n",
        "    logprint(f\"[NCBI] Intento remoto por accesión: {run}\")\n",
        "    if fasterq_dump_remote(run):\n",
        "        for p in list_fastq_for_run(CONV_DIR, run):\n",
        "            logprint(f\"[NCBI] generado (remoto): {p.name}\")\n",
        "        return\n",
        "\n",
        "    # Preferir SRA completo si el TSV lo trae\n",
        "    sra_link  = next((u for u in links if u.lower().endswith(\".sra\")), None)\n",
        "    lite_link = next((u for u in links if u.lower().endswith(\".lite.1\")), None)\n",
        "    sra_path = None\n",
        "\n",
        "    if sra_link:\n",
        "        base = os.path.basename(sra_link)\n",
        "        sra_path = CONV_DIR / base\n",
        "        if not sra_path.exists():\n",
        "            logprint(f\"[NCBI] Descargando archivo del TSV: {base}\")\n",
        "            if not download_from_url(sra_link, sra_path):\n",
        "                logprint(\"[NCBI] Descarga .sra desde NCBI falló.\")\n",
        "                sra_path = None\n",
        "        else:\n",
        "            logprint(f\"[NCBI] Ya existe archivo: {sra_path.name}\")\n",
        "\n",
        "    if sra_path and sra_path.exists() and sra_path.stat().st_size > 0:\n",
        "        if fasterq_dump_local(sra_path, run) or fastq_dump_local(sra_path, run) or fasterq_dump_container(sra_path, run):\n",
        "            for p in list_fastq_for_run(CONV_DIR, run):\n",
        "                logprint(f\"[NCBI] generado (.sra): {p.name}\")\n",
        "            return\n",
        "        logprint(\"[NCBI] Conversión falló con .sra del TSV.\")\n",
        "\n",
        "    # ENA: primero FASTQ (si existen), luego SRA\n",
        "    logprint(\"[NCBI] Intento obtener FASTQ directos desde ENA.\")\n",
        "    if download_fastq_from_ena(run) > 0:\n",
        "        for p in list_fastq_for_run(CONV_DIR, run):\n",
        "            logprint(f\"[NCBI] generado (ENA FASTQ): {p.name}\")\n",
        "        return\n",
        "\n",
        "    logprint(\"[NCBI] Intento obtener .sra completo desde ENA.\")\n",
        "    ena_sra = download_sra_from_ena(run)\n",
        "    if ena_sra and ena_sra.exists() and ena_sra.stat().st_size > 0:\n",
        "        if fasterq_dump_local(ena_sra, run) or fastq_dump_local(ena_sra, run) or fasterq_dump_container(ena_sra, run):\n",
        "            for p in list_fastq_for_run(CONV_DIR, run):\n",
        "                logprint(f\"[NCBI] generado (ENA .sra): {p.name}\")\n",
        "            return\n",
        "\n",
        "    if lite_link:\n",
        "        base = os.path.basename(lite_link)\n",
        "        lite_path = CONV_DIR / base\n",
        "        if not lite_path.exists():\n",
        "            logprint(f\"[NCBI] Descargando archivo .lite.1 del TSV: {base}\")\n",
        "            if not download_from_url(lite_link, lite_path):\n",
        "                lite_path = None\n",
        "        if lite_path and lite_path.exists() and lite_path.stat().st_size > 0:\n",
        "            if fasterq_dump_container(lite_path, run):\n",
        "                for p in list_fastq_for_run(CONV_DIR, run):\n",
        "                    logprint(f\"[NCBI] generado (.lite.1 → contenedor 3.x): {p.name}\")\n",
        "                return\n",
        "\n",
        "    logprint(\"[NCBI] No fue posible generar FASTQ para la corrida.\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# EJECUCIÓN PARA TODAS LAS FILAS\n",
        "# ---------------------------------------------------------------------\n",
        "procesadas = set()\n",
        "ok_ena = ok_ncbi = 0\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    run = str(row.get(COL_RUN, \"\")).strip()\n",
        "    if not run:\n",
        "        continue\n",
        "    if run in EXCLUDE_RUNS:\n",
        "        logprint(f\"[SKIP] EXCLUDE_RUNS: {run}\")\n",
        "        continue\n",
        "    if run in procesadas:\n",
        "        continue\n",
        "    procesadas.add(run)\n",
        "\n",
        "    try:\n",
        "        if is_ena_row(row):\n",
        "            logprint(f\"\\n[PROCESS][ENA] idx={i} Run={run}\")\n",
        "            got = download_ena_row(row)\n",
        "            ok_ena += 1 if got > 0 else 0\n",
        "            # Si el TSV trae .fastq.gz ya no intentamos NCBI para evitar duplicados\n",
        "            continue\n",
        "\n",
        "        if is_ncbi_row(row):\n",
        "            logprint(f\"\\n[PROCESS][NCBI] idx={i} Run={run}\")\n",
        "            download_ncbi_row_and_convert(row)\n",
        "            ok_ncbi += 1\n",
        "        else:\n",
        "            logprint(f\"[WARN] Fila sin enlaces útiles: idx={i} Run={run}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logprint(f\"[ERROR] Fallo procesando idx={i} Run={run}: {e}\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Verificaciones simples\n",
        "# ---------------------------------------------------------------------\n",
        "try:\n",
        "    ena_files = [f for f in os.listdir(ENA_DIR) if f.endswith(\".fastq.gz\")]\n",
        "    print(f\"[CHK][ENA] archivos (directos): {len(ena_files)}\")\n",
        "except Exception as e:\n",
        "    print(\"[CHK][ENA] error:\", e)\n",
        "\n",
        "try:\n",
        "    conv_files = [f for f in os.listdir(CONV_DIR) if f.endswith(\".fastq\") or f.endswith(\".fastq.gz\")]\n",
        "    print(f\"[CHK][NCBI] archivos: {len(conv_files)}\")\n",
        "except Exception as e:\n",
        "    print(\"[CHK][NCBI] error:\", e)\n",
        "\n",
        "print(f\"Procesadas ENA: {ok_ena}, NCBI: {ok_ncbi}, Total RUNs únicos: {len(procesadas)}\")\n",
        "print(\"Proceso ENA + NCBI para TODAS las muestras finalizado.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ac4047b",
      "metadata": {
        "id": "1ac4047b"
      },
      "source": [
        "# Descarga y conversión ENA/NCBI\n",
        "\n",
        "- Procesa todas las filas del **TSV**.  \n",
        "- Descarga **FASTQ** desde **ENA** cuando hay `.fastq.gz`.  \n",
        "- Si no hay, convierte desde **NCBI**, intentando en orden:\n",
        "  1. `fasterq-dump <RUN>` remoto  \n",
        "  2. Descarga y conversión de `.sra` (ENA/NCBI)  \n",
        "  3. Conversión de `.lite.1` con contenedor **sra-tools 3.x** (si existe)  \n",
        "- Normaliza nombres: `<RUN>_1.fastq` y opcional `<RUN>_2.fastq`.\n",
        "\n",
        "---\n",
        "\n",
        "## Entradas\n",
        "- **TSV**: `metadata/metadatos_unificados.tsv` con columnas:\n",
        "  - `Run` / `run_accession`\n",
        "  - `fastq_ftp`, `fastq_http`\n",
        "  - `sra_ftp`, `sra_http`\n",
        "  - `ftp_link`\n",
        "  - `download_path`\n",
        "- **CA opcional**: `CORP_CA`\n",
        "\n",
        "---\n",
        "\n",
        "## Salidas\n",
        "- **FASTQ** en `rawdata/convertidos/`  \n",
        "  - *Paired*: `<RUN>_1.fastq[.gz]`, `<RUN>_2.fastq[.gz]`  \n",
        "  - *Single*: `<RUN>_1.fastq[.gz]`  \n",
        "- Descargas **ENA** en `rawdata/ena/`  \n",
        "- **Logs** en: `logs/descarga_all_YYYYmmdd_HHMMSS.log`  \n",
        "- Temporales en: `tmp_fqd/`\n",
        "\n",
        "---\n",
        "\n",
        "## Parámetros editables\n",
        "- `THREADS`  \n",
        "- `KEEP_SRA`  \n",
        "- `EXCLUDE_RUNS`  \n",
        "- `ALLOW_INSECURE_WGET`  \n",
        "- `TOUCH_EMPTY_R2`  \n",
        "- `CORP_CA`  \n",
        "\n",
        "### Rutas base\n",
        "- `PROJECT_DIR`  \n",
        "- `ENA_DIR`  \n",
        "- `CONV_DIR`  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d65921f7",
      "metadata": {
        "id": "d65921f7"
      },
      "source": [
        "# 3) Limpieza y enriquecimiento de metadatos (centrado en coordenadas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8be366",
      "metadata": {
        "id": "ff8be366"
      },
      "source": [
        "## 3.1 Altitud (SRTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4706205",
      "metadata": {
        "id": "c4706205"
      },
      "outputs": [],
      "source": [
        "import elevation\n",
        "\n",
        "INPUT_TSV = OUT_DIR / \"metadatos_unificados.tsv\"\n",
        "OUTPUT_TSV= OUT_DIR / \"metadatos_con_altitud.tsv\"\n",
        "BUFFER    = 1.0\n",
        "\n",
        "df = pd.read_csv(INPUT_TSV, sep=\"\\t\")\n",
        "lat_min, lat_max = df[\"latitude\"].astype(float).min()-BUFFER, df[\"latitude\"].astype(float).max()+BUFFER\n",
        "lon_min, lon_max = df[\"longitude\"].astype(float).min()-BUFFER, df[\"longitude\"].astype(float).max()+BUFFER\n",
        "\n",
        "DEM_TIF.parent.mkdir(parents=True, exist_ok=True)\n",
        "elevation.clip(bounds=(lon_min, lat_min, lon_max, lat_max), output=str(DEM_TIF))\n",
        "elevation.clean()\n",
        "\n",
        "vals = []\n",
        "with rasterio.open(DEM_TIF) as src:\n",
        "    for _, r in df.iterrows():\n",
        "        lon, lat = float(r[\"longitude\"]), float(r[\"latitude\"])\n",
        "        x, y = rio_transform({'init': 'EPSG:4326'}, src.crs, [lon], [lat])\n",
        "        v = next(src.sample([(x[0], y[0])]))[0]\n",
        "        vals.append(float(v) if v is not None else np.nan)\n",
        "\n",
        "df[\"altitude_m\"] = vals\n",
        "save_tsv(df, OUTPUT_TSV)\n",
        "print(\"Altitud añadida ->\", OUTPUT_TSV)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390a1618",
      "metadata": {
        "id": "390a1618"
      },
      "source": [
        "**Resumen del bloque — SRTM**  \n",
        "**Qué hace:** recorta DEM y extrae altitud por coordenada.  \n",
        "**Entradas:** `outputs/metadatos_con_coord.tsv`, DEM `srtm.tif`.  \n",
        "**Salidas:** `outputs/metadatos_con_altitud.tsv`.  \n",
        "**Parámetros editables:** `BUFFER`, `DEM_TIF`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79318f88",
      "metadata": {
        "id": "79318f88"
      },
      "source": [
        "## 3.3 WorldClim 2.1 (BIO1–BIO19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d79f34c0",
      "metadata": {
        "id": "d79f34c0"
      },
      "outputs": [],
      "source": [
        "INPUT_TSV  = OUT_DIR / \"metadatos_con_altitud.tsv\"\n",
        "OUTPUT_TSV = OUT_DIR / \"metadatos_con_clima.tsv\"\n",
        "\n",
        "df = pd.read_csv(INPUT_TSV, sep=\"\\t\")\n",
        "for i in range(1, 20):\n",
        "    df[f\"bio{i}\"] = np.nan\n",
        "\n",
        "def extract_from_raster(src, lon, lat):\n",
        "    try:\n",
        "        row, col = src.index(lon, lat)\n",
        "        return float(src.read(1)[row, col])\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "for i in range(1, 19+1):\n",
        "    tif = CLIM_DIR / f\"wc2.1_30s_bio_{i}.tif\"\n",
        "    if not tif.exists():\n",
        "        print(\"Falta\", tif, \"omitiendo bio\", i)\n",
        "        continue\n",
        "    with rasterio.open(tif) as src:\n",
        "        for idx,(lon,lat) in enumerate(tqdm(zip(df[\"longitude\"].astype(float), df[\"latitude\"].astype(float)), total=len(df), desc=f\"bio{i}\")):\n",
        "            v = extract_from_raster(src, float(lon), float(lat))\n",
        "            if not np.isfinite(v):\n",
        "                arr = src.read(1)\n",
        "                r,c = src.index(float(lon), float(lat))\n",
        "                r0, r1 = max(0, r-10), min(arr.shape[0], r+11)\n",
        "                c0, c1 = max(0, c-10), min(arr.shape[1], c+11)\n",
        "                window = arr[r0:r1, c0:c1]\n",
        "                valid = window[np.isfinite(window)]\n",
        "                v = float(valid[0]) if valid.size>0 else np.nan\n",
        "            df.at[idx, f\"bio{i}\"] = v\n",
        "\n",
        "save_tsv(df, OUTPUT_TSV)\n",
        "print(\"WorldClim añadido ->\", OUTPUT_TSV)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc136127",
      "metadata": {
        "id": "bc136127"
      },
      "source": [
        "**Resumen del bloque — WorldClim**  \n",
        "**Qué hace:** extrae BIO1–BIO19 por coordenada con fallback local.  \n",
        "**Entradas:** `outputs/metadatos_con_altitud.tsv`, `wc2.1_30s_bio_*.tif`.  \n",
        "**Salidas:** `outputs/metadatos_con_clima.tsv`.  \n",
        "**Parámetros editables:** `CLIM_DIR`, vecindad.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c07e76",
      "metadata": {
        "id": "33c07e76"
      },
      "source": [
        "## 3.4 Ecorregiones marinas (MEOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02165017",
      "metadata": {
        "id": "02165017"
      },
      "outputs": [],
      "source": [
        "INPUT_TSV  = OUT_DIR / \"metadatos_con_clima.tsv\"\n",
        "OUTPUT_TSV = OUT_DIR / \"metadatos_con_ecoregion_marina.tsv\"\n",
        "\n",
        "df = pd.read_csv(INPUT_TSV, sep=\"\\t\")\n",
        "gdf_points = gpd.GeoDataFrame(\n",
        "    df.copy(),\n",
        "    geometry=[Point(xy) for xy in zip(df[\"longitude\"].astype(float), df[\"latitude\"].astype(float))],\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "if not MEOW_SHP.exists():\n",
        "    raise FileNotFoundError(f\"No se encontró el shapefile MEOW en {MEOW_SHP}. Ajuste la ruta MEOW_SHP.\")\n",
        "gdf_meow = gpd.read_file(MEOW_SHP).to_crs(\"EPSG:4326\")\n",
        "\n",
        "joined = gpd.sjoin(\n",
        "    gdf_points,\n",
        "    gdf_meow[[\"ECOREGION\", \"PROVINCE\", \"REALM\", \"geometry\"]],\n",
        "    how=\"left\",\n",
        "    predicate=\"intersects\"\n",
        ")\n",
        "\n",
        "out = joined.drop(columns=\"geometry\")\n",
        "save_tsv(out, OUTPUT_TSV)\n",
        "print(\"MEOW añadido ->\", OUTPUT_TSV)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e889e8e1",
      "metadata": {
        "id": "e889e8e1"
      },
      "source": [
        "**Resumen del bloque — MEOW**  \n",
        "**Qué hace:** crea `GeoDataFrame` de puntos y une con MEOW para asignar ecorregión/provincia/reino.  \n",
        "**Entradas:** `outputs/metadatos_con_clima.tsv`, shapefile `MEOW`.  \n",
        "**Salidas:** `outputs/metadatos_con_ecoregion_marina.tsv`.  \n",
        "**Parámetros editables:** `MEOW_SHP`, `predicate`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d880c5",
      "metadata": {
        "id": "d3d880c5"
      },
      "source": [
        "## 3.5 Huella humana (HII 2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407232d5",
      "metadata": {
        "id": "407232d5"
      },
      "outputs": [],
      "source": [
        "INPUT_TSV = OUT_DIR / \"metadatos_con_ecoregion_marina.tsv\"\n",
        "OUTPUT_TSV= OUT_DIR / \"metadatos_con_hii.tsv\"\n",
        "\n",
        "if not HII_TIF.exists():\n",
        "    raise FileNotFoundError(f\"No se encontró el raster HII en {HII_TIF}. Ajuste la ruta HII_TIF.\")\n",
        "\n",
        "df = pd.read_csv(INPUT_TSV, sep=\"\\t\")\n",
        "vals = []\n",
        "with rasterio.open(HII_TIF) as src:\n",
        "    arr = src.read(1)\n",
        "    nodata = src.nodata\n",
        "    for lon, lat in tqdm(zip(df[\"longitude\"].astype(float), df[\"latitude\"].astype(float)), total=len(df), desc=\"Extrayendo HII\"):\n",
        "        try:\n",
        "            r, c = src.index(float(lon), float(lat))\n",
        "            if 0 <= r < arr.shape[0] and 0 <= c < arr.shape[1]:\n",
        "                v = arr[r, c]\n",
        "            else:\n",
        "                v = np.nan\n",
        "            if (nodata is not None and v == nodata) or not np.isfinite(v):\n",
        "                r0, r1 = max(0, r-10), min(arr.shape[0], r+11)\n",
        "                c0, c1 = max(0, c-10), min(arr.shape[1], c+11)\n",
        "                sub = arr[r0:r1, c0:c1]\n",
        "                valid = sub[(sub != nodata) & np.isfinite(sub)]\n",
        "                v = valid[0] if valid.size > 0 else np.nan\n",
        "            vals.append(float(v) if np.isfinite(v) else np.nan)\n",
        "        except Exception:\n",
        "            vals.append(np.nan)\n",
        "\n",
        "df[\"human_footprint_2020\"] = vals\n",
        "save_tsv(df, OUTPUT_TSV)\n",
        "print(\"HII añadido ->\", OUTPUT_TSV)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e304d10c",
      "metadata": {
        "id": "e304d10c"
      },
      "source": [
        "**Resumen del bloque — HII**  \n",
        "**Qué hace:** extrae huella humana para cada coordenada con manejo de NoData y fallback local.  \n",
        "**Entradas:** `outputs/metadatos_con_ecoregion_marina.tsv`, raster `hii_2020.tif`.  \n",
        "**Salidas:** `outputs/metadatos_con_hii.tsv`.  \n",
        "**Parámetros editables:** `HII_TIF`, tamaño de vecindad.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c118626",
      "metadata": {
        "id": "3c118626"
      },
      "source": [
        "## 3.6 Suelos — SoilGrids v2.0 (0–30 cm) + WRB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcd49733",
      "metadata": {
        "id": "dcd49733"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "from typing import Dict, Tuple, Any, Optional, List\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "INPUT_TSV  = OUT_DIR / \"metadatos_con_hii.tsv\"\n",
        "OUTPUT_TSV = OUT_DIR / \"metadatos_con_suelos.tsv\"\n",
        "\n",
        "ALL_PROPS = [\"bdod\", \"cec\", \"clay\", \"sand\", \"silt\", \"phh2o\", \"soc\", \"nitrogen\", \"cfvo\", \"ocs\"]\n",
        "DEPTHS    = [\"0-5cm\",\"5-15cm\",\"15-30cm\"]\n",
        "VALUES    = [\"mean\",\"Q0.5\"]\n",
        "\n",
        "REQUESTS_PER_SEC = 6\n",
        "SLEEP_BETWEEN = 1.0 / REQUESTS_PER_SEC\n",
        "\n",
        "MAX_RETRIES = 4\n",
        "RETRY_BACKOFF = [1.0, 2.0, 4.0, 8.0]\n",
        "\n",
        "SOIL_ENDPOINTS = [\n",
        "    \"https://rest.isric.org/soilgrids/v2.0/properties/query\",\n",
        "    \"https://api.soilgrids.org/soilgrids/v2.0/properties/query\",\n",
        "]\n",
        "\n",
        "CONNECT_TIMEOUT_S = 8\n",
        "READ_TIMEOUT_S = 20\n",
        "\n",
        "\n",
        "RADIUSES_KM = [0.0, 0.5, 1.0, 1.5, 2.0]\n",
        "DIRECTIONS_DEG = [0, 45, 90, 135, 180, 225, 270, 315]\n",
        "\n",
        "ROUND_DECIMALS: Optional[int] = None  # p.ej., 4 => ~11 m\n",
        "\n",
        "def move_point(lat: float, lon: float, distance_km: float, bearing_deg: float) -> Tuple[float, float]:\n",
        "    R = 6371.0\n",
        "    b = math.radians(bearing_deg)\n",
        "    lat1 = math.radians(lat); lon1 = math.radians(lon)\n",
        "    d = distance_km / R\n",
        "    lat2 = math.asin(math.sin(lat1)*math.cos(d) + math.cos(lat1)*math.sin(d)*math.cos(b))\n",
        "    lon2 = lon1 + math.atan2(math.sin(b)*math.sin(d)*math.cos(lat1), math.cos(d) - math.sin(lat1)*math.sin(lat2))\n",
        "    return (math.degrees(lat2), (math.degrees(lon2) + 540) % 360 - 180)\n",
        "\n",
        "def pick_endpoint() -> str:\n",
        "    return SOIL_ENDPOINTS[int(time.time()) % len(SOIL_ENDPOINTS)]\n",
        "\n",
        "_last_request_time = 0.0\n",
        "def throttle():\n",
        "    global _last_request_time\n",
        "    now = time.time()\n",
        "    wait = SLEEP_BETWEEN - (now - _last_request_time)\n",
        "    if wait > 0:\n",
        "        time.sleep(wait)\n",
        "    _last_request_time = time.time()\n",
        "\n",
        "def build_params(lat: float, lon: float, properties: List[str]) -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"lat\": float(lat),\n",
        "        \"lon\": float(lon),\n",
        "        \"property\": \",\".join(properties),\n",
        "        \"depth\": \",\".join(DEPTHS),\n",
        "        \"value\": \",\".join(VALUES),\n",
        "        \"prefix\": \"mean\",\n",
        "    }\n",
        "\n",
        "def do_request(params: Dict[str, Any]) -> Optional[requests.Response]:\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        throttle()\n",
        "        try:\n",
        "            resp = requests.get(pick_endpoint(), params=params, timeout=(CONNECT_TIMEOUT_S, READ_TIMEOUT_S))\n",
        "            if resp.status_code == 200:\n",
        "                return resp\n",
        "            if 500 <= resp.status_code < 600:\n",
        "                time.sleep(RETRY_BACKOFF[min(attempt, len(RETRY_BACKOFF)-1)])\n",
        "                continue\n",
        "            return resp\n",
        "        except requests.RequestException:\n",
        "            time.sleep(RETRY_BACKOFF[min(attempt, len(RETRY_BACKOFF)-1)])\n",
        "    return None\n",
        "\n",
        "def _depth_variants(depth_label: str) -> List[str]:\n",
        "    return [depth_label, depth_label.replace(\"-\", \"_\")]\n",
        "\n",
        "def any_numeric_value(props: Dict[str, Any], prop: str, depth_label: str) -> bool:\n",
        "    for d in _depth_variants(depth_label):\n",
        "        for stat in VALUES:\n",
        "            v = props.get(f\"{prop}_{d}_{stat}\")\n",
        "            if isinstance(v, (int, float)):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def extract_value(props: Dict[str, Any], prop: str, depth_label: str) -> Optional[float]:\n",
        "\n",
        "    for d in _depth_variants(depth_label):\n",
        "        for stat in VALUES:\n",
        "            v = props.get(f\"{prop}_{d}_{stat}\")\n",
        "            if isinstance(v, (int, float)):\n",
        "                return float(v)\n",
        "    return None\n",
        "\n",
        "def probe_cell_has_numeric(lat: float, lon: float) -> bool:\n",
        "    params = build_params(lat, lon, [\"clay\"])\n",
        "    resp = do_request(params)\n",
        "    if resp is None or resp.status_code != 200:\n",
        "        return False\n",
        "    try:\n",
        "        data = resp.json()\n",
        "        props = data.get(\"properties\", {})\n",
        "    except Exception:\n",
        "        return False\n",
        "    for d in DEPTHS:\n",
        "        if any_numeric_value(props, \"clay\", d):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def fetch_all_properties(lat: float, lon: float, props_list: List[str]) -> Optional[Dict[str, Any]]:\n",
        "    params = build_params(lat, lon, props_list)\n",
        "    resp = do_request(params)\n",
        "    if resp is None or resp.status_code != 200:\n",
        "        return None\n",
        "    try:\n",
        "        return resp.json()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def enrich_coord_with_fallback(lat: float, lon: float, props_list: List[str]) -> Optional[Dict[str, Any]]:\n",
        "    for radius in RADIUSES_KM:\n",
        "        candidates = [(lat, lon)] if radius == 0 else [move_point(lat, lon, radius, b) for b in DIRECTIONS_DEG]\n",
        "        for lt, ln in candidates:\n",
        "            if not (-90 <= lt <= 90 and -180 <= ln <= 180):\n",
        "                continue\n",
        "            if not probe_cell_has_numeric(lt, ln):\n",
        "                continue\n",
        "            data = fetch_all_properties(lt, ln, props_list)\n",
        "            if data is not None:\n",
        "                return data\n",
        "    return None\n",
        "\n",
        "df = pd.read_csv(INPUT_TSV, sep=\"\\t\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "for p in ALL_PROPS:\n",
        "    for d in DEPTHS:\n",
        "        col = f\"{p}_{d}_mean\"\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "\n",
        "\n",
        "def coord_key(lat: float, lon: float) -> Tuple[float, float]:\n",
        "    if ROUND_DECIMALS is not None:\n",
        "        return (round(lat, ROUND_DECIMALS), round(lon, ROUND_DECIMALS))\n",
        "    return (lat, lon)\n",
        "\n",
        "cache: Dict[Tuple[float, float], Optional[Dict[str, Any]]] = {}\n",
        "\n",
        "lats = df[\"latitude\"].astype(float).to_numpy()\n",
        "lons = df[\"longitude\"].astype(float).to_numpy()\n",
        "\n",
        "unique_keys = []\n",
        "seen = set()\n",
        "for lt, ln in zip(lats, lons):\n",
        "    k = coord_key(lt, ln)\n",
        "    if k not in seen:\n",
        "        seen.add(k)\n",
        "        unique_keys.append(k)\n",
        "\n",
        "for (lt, ln) in tqdm(unique_keys, desc=\"SoilGrids (props/dep/values actualizados)\"):\n",
        "    cache[(lt, ln)] = enrich_coord_with_fallback(lt, ln, ALL_PROPS)\n",
        "\n",
        "\n",
        "for idx, (lt, ln) in enumerate(zip(lats, lons)):\n",
        "    k = coord_key(lt, ln)\n",
        "    data = cache.get(k)\n",
        "    if not data:\n",
        "        continue\n",
        "    props_dict = data.get(\"properties\", {}) if isinstance(data, dict) else {}\n",
        "    for p in ALL_PROPS:\n",
        "        for d in DEPTHS:\n",
        "            col = f\"{p}_{d}_mean\"\n",
        "            v = extract_value(props_dict, p, d)\n",
        "            if v is not None:\n",
        "                df.at[idx, col] = v\n",
        "\n",
        "save_tsv(df, OUTPUT_TSV)\n",
        "print(\"SoilGrids añadido ->\", OUTPUT_TSV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a00498d",
      "metadata": {
        "id": "7a00498d"
      },
      "source": [
        "**Resumen del bloque — SoilGrids + WRB**  \n",
        "**Qué hace:** consulta SoilGrids por punto con control de tasa y reintentos; agrega propiedades y WRB si disponible.  \n",
        "**Entradas:** `outputs/metadatos_con_hii.tsv`, API SoilGrids.  \n",
        "**Salidas:** `outputs/metadatos_con_suelos.tsv`.  \n",
        "**Parámetros editables:** `PROPERTIES`, `DEPTHS`, `MAX_WORKERS`, `REQUESTS_PER_SEC`, `RETRIES`, `BACKOFF`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c1907e4",
      "metadata": {
        "id": "0c1907e4"
      },
      "source": [
        "## 3.7 GBIF — especies de manglar y biodiversidad de plantas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f1d09c",
      "metadata": {
        "id": "62f1d09c"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import time, math, threading\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "INPUT_TSV  = OUT_DIR / \"metadatos_con_suelos.tsv\"\n",
        "OUTPUT_TSV = OUT_DIR / \"metadatos_con_gbif.tsv\"\n",
        "\n",
        "RADIUS_M = 2000\n",
        "KINGDOM_KEY = None\n",
        "YEAR_FROM = None\n",
        "YEAR_TO   = None\n",
        "\n",
        "GBIF_OCC_URL   = \"https://api.gbif.org/v1/occurrence/search\"\n",
        "GBIF_TAXON_URL = \"https://api.gbif.org/v1/species/\"\n",
        "\n",
        "\n",
        "PAGE_LIMIT = 300\n",
        "MAX_PAGES_PER_POINT = 50\n",
        "MAX_RECORDS_PER_POINT = PAGE_LIMIT * MAX_PAGES_PER_POINT\n",
        "\n",
        "\n",
        "MAX_WORKERS = 8\n",
        "GLOBAL_RPS = 3\n",
        "MAX_BACKOFF_SEC = 60\n",
        "\n",
        "_print_lock = threading.Lock()\n",
        "def log(msg: str):\n",
        "    with _print_lock:\n",
        "        print(msg, flush=True)\n",
        "\n",
        "class RateLimiter:\n",
        "    def __init__(self, rate_per_sec: float, capacity: int | None = None):\n",
        "        self.rate = float(rate_per_sec)\n",
        "        self.capacity = int(capacity or max(1, int(rate_per_sec * 2)))\n",
        "        self.tokens = self.capacity\n",
        "        self.lock = threading.Lock()\n",
        "        self.ts = time.time()\n",
        "    def acquire(self):\n",
        "        while True:\n",
        "            with self.lock:\n",
        "                now = time.time()\n",
        "                refill = (now - self.ts) * self.rate\n",
        "                if refill > 0:\n",
        "                    self.tokens = min(self.capacity, self.tokens + refill)\n",
        "                    self.ts = now\n",
        "                if self.tokens >= 1:\n",
        "                    self.tokens -= 1\n",
        "                    return\n",
        "            time.sleep(0.01)\n",
        "\n",
        "RATE = RateLimiter(GLOBAL_RPS)\n",
        "\n",
        "def make_session() -> requests.Session:\n",
        "    s = requests.Session()\n",
        "    ad = requests.adapters.HTTPAdapter(pool_connections=MAX_WORKERS*2, pool_maxsize=MAX_WORKERS*2, max_retries=0)\n",
        "    s.mount(\"https://\", ad); s.mount(\"http://\", ad)\n",
        "    s.headers.update({\"User-Agent\": \"MAGENTA-GBIF-diversity/2km/1.0 (+local)\"})\n",
        "    return s\n",
        "\n",
        "SESSION = make_session()\n",
        "\n",
        "def _get(url: str, params: Dict[str, Any], max_attempts=8) -> Optional[requests.Response]:\n",
        "    \"\"\"GET con RPS global y backoff exponencial (+ Retry-After cuando esté presente).\"\"\"\n",
        "    attempt = 0; backoff = 1.0\n",
        "    while attempt < max_attempts:\n",
        "        attempt += 1\n",
        "        RATE.acquire()\n",
        "        try:\n",
        "            r = SESSION.get(url, params=params, timeout=30)\n",
        "        except requests.RequestException:\n",
        "            time.sleep(min(backoff, MAX_BACKOFF_SEC))\n",
        "            backoff = min(MAX_BACKOFF_SEC, backoff * 2) + 0.1 * attempt\n",
        "            continue\n",
        "        if r.status_code == 200:\n",
        "            return r\n",
        "        if r.status_code in (429, 500, 502, 503, 504):\n",
        "            ra = r.headers.get(\"Retry-After\")\n",
        "            wait = float(ra) if ra and str(ra).isdigit() else backoff\n",
        "            time.sleep(min(wait, MAX_BACKOFF_SEC))\n",
        "            backoff = min(MAX_BACKOFF_SEC, backoff * 2) + 0.1 * attempt\n",
        "            continue\n",
        "\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "_taxon_cache: Dict[int, str | None] = {}\n",
        "\n",
        "def gbif_species_name(species_key: int) -> Optional[str]:\n",
        "    \"\"\"Resuelve nombre científico desde speciesKey, con caché simple.\"\"\"\n",
        "    if species_key in _taxon_cache:\n",
        "        return _taxon_cache[species_key]\n",
        "    r = _get(f\"{GBIF_TAXON_URL}{species_key}\", {})\n",
        "    if not r:\n",
        "        _taxon_cache[species_key] = None\n",
        "        return None\n",
        "    try:\n",
        "        js = r.json()\n",
        "        name = js.get(\"scientificName\") or js.get(\"canonicalName\")\n",
        "    except Exception:\n",
        "        name = None\n",
        "    _taxon_cache[species_key] = name\n",
        "    return name\n",
        "\n",
        "def _apply_common_filters(q: Dict[str, Any]):\n",
        "    q[\"hasCoordinate\"] = \"true\"\n",
        "    q[\"hasGeospatialIssue\"] = \"false\"\n",
        "    q[\"occurrenceStatus\"] = \"PRESENT\"\n",
        "    if KINGDOM_KEY is not None:\n",
        "        q[\"kingdomKey\"] = KINGDOM_KEY\n",
        "    if YEAR_FROM is not None:\n",
        "        q[\"year\"] = f\"{YEAR_FROM},{YEAR_TO or ''}\".strip(\",\")\n",
        "\n",
        "def _bbox_wkt(lat: float, lon: float, radius_m: int) -> str:\n",
        "    \"\"\"BBox aproximado en grados a partir de radio en metros (depende de latitud).\"\"\"\n",
        "    dlat = radius_m / 1000.0 / 111.32\n",
        "    dlon = dlat / max(0.1, math.cos(math.radians(lat)))\n",
        "    minx = lon - dlon; maxx = lon + dlon\n",
        "    miny = lat - dlat; maxy = lat + dlat\n",
        "    return f\"POLYGON(({minx:.6f} {miny:.6f},{maxx:.6f} {miny:.6f},{maxx:.6f} {maxy:.6f},{minx:.6f} {maxy:.6f},{minx:.6f} {miny:.6f}))\"\n",
        "\n",
        "def _page_species_counts(params: Dict[str, Any]) -> Tuple[int, Counter]:\n",
        "    \"\"\"\n",
        "    Pagina occurrence/search sumando speciesKey (o taxonKey) en cliente.\n",
        "    Devuelve (total_reported, species_counter).\n",
        "    \"\"\"\n",
        "    p0 = dict(params); p0[\"limit\"] = 0\n",
        "    total_reported = 0\n",
        "    r0 = _get(GBIF_OCC_URL, p0)\n",
        "    if r0:\n",
        "        try:\n",
        "            total_reported = int(r0.json().get(\"count\", 0))\n",
        "        except Exception:\n",
        "            total_reported = 0\n",
        "\n",
        "    species_counter: Counter = Counter()\n",
        "    fetched = 0\n",
        "    offset = 0\n",
        "    pages = 0\n",
        "\n",
        "    while fetched < min(total_reported, MAX_RECORDS_PER_POINT) and pages < MAX_PAGES_PER_POINT:\n",
        "        q = dict(params)\n",
        "        q[\"limit\"] = PAGE_LIMIT\n",
        "        q[\"offset\"] = offset\n",
        "        r = _get(GBIF_OCC_URL, q)\n",
        "        if not r:\n",
        "            break\n",
        "        try:\n",
        "            js = r.json()\n",
        "        except Exception:\n",
        "            break\n",
        "        results = js.get(\"results\", [])\n",
        "        if not results:\n",
        "            break\n",
        "        for rec in results:\n",
        "            sk = rec.get(\"speciesKey\") or rec.get(\"taxonKey\")\n",
        "            if isinstance(sk, int):\n",
        "                species_counter[sk] += 1\n",
        "        n = len(results)\n",
        "        fetched += n\n",
        "        offset += n\n",
        "        pages += 1\n",
        "        if n < PAGE_LIMIT:\n",
        "            break\n",
        "\n",
        "    return total_reported, species_counter\n",
        "\n",
        "def gbif_occurrence_counts_spatial(lat: float, lon: float, radius_m: int) -> Tuple[int, List[Tuple[int,int]], str]:\n",
        "    \"\"\"\n",
        "    Devuelve (total_occurrences, [(speciesKey, count), ...], spatial_mode)\n",
        "      A) geo_distance  (snake_case)\n",
        "      B) geoDistance   (camelCase)\n",
        "      C) geometry=BBOX (WKT)\n",
        "    \"\"\"\n",
        "    baseA = {}\n",
        "    _apply_common_filters(baseA)\n",
        "    baseA[\"geo_distance\"] = f\"{radius_m}m,{lat:.6f},{lon:.6f}\"\n",
        "    total, counter = _page_species_counts(baseA)\n",
        "    if total > 0 and counter:\n",
        "        items = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "        return total, items, \"geo_distance\"\n",
        "\n",
        "    baseB = {}\n",
        "    _apply_common_filters(baseB)\n",
        "    baseB[\"geoDistance\"] = f\"{radius_m}m,{lat:.6f},{lon:.6f}\"\n",
        "    total, counter = _page_species_counts(baseB)\n",
        "    if total > 0 and counter:\n",
        "        items = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "        return total, items, \"geoDistance\"\n",
        "\n",
        "    baseC = {}\n",
        "    _apply_common_filters(baseC)\n",
        "    baseC[\"geometry\"] = _bbox_wkt(lat, lon, radius_m)\n",
        "    total, counter = _page_species_counts(baseC)\n",
        "    items = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "    return total, items, \"bbox_wkt\" if total > 0 else \"none\"\n",
        "\n",
        "def diversity_metrics(species_counts: List[int], label_suffix: str) -> Dict[str, float]:\n",
        "    out = {f\"gbif_richness_species_{label_suffix}\": 0.0,\n",
        "           f\"gbif_shannon_H_{label_suffix}\": np.nan,\n",
        "           f\"gbif_pielou_J_{label_suffix}\": np.nan,\n",
        "           f\"gbif_simpson_1minD_{label_suffix}\": np.nan,\n",
        "           f\"gbif_chao1_{label_suffix}\": np.nan}\n",
        "    S = int(sum(1 for c in species_counts if c > 0))\n",
        "    out[f\"gbif_richness_species_{label_suffix}\"] = float(S)\n",
        "    N = float(sum(species_counts))\n",
        "    if S == 0 or N <= 0:\n",
        "        return out\n",
        "    p = [c / N for c in species_counts if c > 0]\n",
        "    H = -sum(pi * math.log(pi) for pi in p)\n",
        "    out[f\"gbif_shannon_H_{label_suffix}\"] = float(H)\n",
        "    out[f\"gbif_pielou_J_{label_suffix}\"] = float(H / math.log(S)) if S > 1 else np.nan\n",
        "    D = sum(pi * pi for pi in p)\n",
        "    out[f\"gbif_simpson_1minD_{label_suffix}\"] = float(1.0 - D)\n",
        "    F1 = sum(1 for c in species_counts if c == 1)\n",
        "    F2 = sum(1 for c in species_counts if c == 2)\n",
        "    chao1 = (S + (F1 * F1) / (2.0 * F2)) if F2 > 0 else (S + (F1 * (F1 - 1)) / 2.0)\n",
        "    out[f\"gbif_chao1_{label_suffix}\"] = float(chao1)\n",
        "    return out\n",
        "\n",
        "def fetch_point_fixed_2km(lat: float, lon: float) -> Dict[str, Any]:\n",
        "    out: Dict[str, Any] = {\n",
        "        \"gbif_occurrences_effective\": 0,\n",
        "        \"gbif_top_species_effective\": None,\n",
        "        \"gbif_scope\": (\"All\" if KINGDOM_KEY is None else str(KINGDOM_KEY)),\n",
        "        \"gbif_radius_effective_m\": RADIUS_M,\n",
        "        \"gbif_spatial_mode\": None,\n",
        "        \"gbif_error\": None\n",
        "    }\n",
        "\n",
        "    total, items, spatial_mode = gbif_occurrence_counts_spatial(lat, lon, RADIUS_M)\n",
        "    out[\"gbif_spatial_mode\"] = spatial_mode\n",
        "    out[\"gbif_occurrences_effective\"] = int(total) if total else 0\n",
        "\n",
        "    counts = [c for _, c in items] if items else []\n",
        "    out.update(diversity_metrics(counts, \"2km\"))\n",
        "\n",
        "    top = (items or [])[:5]\n",
        "    names = []\n",
        "    for sk, c in top:\n",
        "        nm = gbif_species_name(sk)\n",
        "        if nm:\n",
        "            names.append(f\"{nm} ({c})\")\n",
        "    if names:\n",
        "        out[\"gbif_top_species_effective\"] = \"; \".join(names)\n",
        "\n",
        "    return out\n",
        "\n",
        "def _timed_fetch(lat: float, lon: float):\n",
        "    t0 = time.time()\n",
        "    out = fetch_point_fixed_2km(lat, lon)\n",
        "    return out, time.time() - t0\n",
        "\n",
        "df = pd.read_csv(INPUT_TSV, sep=\"\\t\").reset_index(drop=True)\n",
        "if not {\"latitude\",\"longitude\"}.issubset(df.columns):\n",
        "    raise ValueError(\"Se requieren columnas 'latitude' y 'longitude' en el TSV de entrada.\")\n",
        "\n",
        "idxs = [i for i in range(len(df)) if pd.notnull(df.at[i, \"latitude\"]) and pd.notnull(df.at[i, \"longitude\"])]\n",
        "\n",
        "features_by_idx: Dict[int, Dict[str, Any]] = {}\n",
        "completed = 0\n",
        "total = len(idxs)\n",
        "log(f\"GBIF diversidad (ALL taxa) **FIJO 2 km** | filas={total} | workers={MAX_WORKERS} | rps={GLOBAL_RPS} | radius={RADIUS_M} m\")\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    futs = {ex.submit(_timed_fetch, float(df.at[i, \"latitude\"]), float(df.at[i, \"longitude\"])): i for i in idxs}\n",
        "    for fut in as_completed(futs):\n",
        "        i = futs[fut]\n",
        "        try:\n",
        "            row, elapsed = fut.result()\n",
        "            ok = \"OK\"\n",
        "        except Exception as e:\n",
        "            row = {\"gbif_error\": str(e),\n",
        "                   \"gbif_occurrences_effective\": 0,\n",
        "                   \"gbif_scope\": (\"All\" if KINGDOM_KEY is None else str(KINGDOM_KEY)),\n",
        "                   \"gbif_radius_effective_m\": RADIUS_M,\n",
        "                   \"gbif_spatial_mode\": \"error\"}\n",
        "            ok = f\"ERR:{e}\"\n",
        "            elapsed = np.nan\n",
        "\n",
        "        features_by_idx[i] = row\n",
        "        lat = float(df.at[i, \"latitude\"]); lon = float(df.at[i, \"longitude\"])\n",
        "        sfx = \"2km\"\n",
        "        log(f\"[{completed+1}/{total}] lat={lat:.5f} lon={lon:.5f} \"\n",
        "            f\"S={row.get(f'gbif_richness_species_{sfx}', 0)} \"\n",
        "            f\"H'={row.get(f'gbif_shannon_H_{sfx}', np.nan)} \"\n",
        "            f\"occ={row.get('gbif_occurrences_effective', 0)} \"\n",
        "            f\"r_eff={row.get('gbif_radius_effective_m')} mode={row.get('gbif_spatial_mode')} {ok} | {elapsed:.2f}s\")\n",
        "        completed += 1\n",
        "\n",
        "feat_df = pd.DataFrame.from_dict(features_by_idx, orient=\"index\").reindex(df.index)\n",
        "out_df = pd.concat([df, feat_df], axis=1)\n",
        "save_tsv(out_df, OUTPUT_TSV)\n",
        "print(\"GBIF añadido ->\", OUTPUT_TSV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad2c70c9",
      "metadata": {
        "id": "ad2c70c9"
      },
      "source": [
        "**Resumen del bloque — GBIF (manglar y plantas)**  \n",
        "**Qué hace:** calcula riqueza de plantas y riqueza/top-5 de especies de manglar por coordenada.  \n",
        "**Entradas:** `outputs/metadatos_con_suelos.tsv`, API GBIF.  \n",
        "**Salidas:** `outputs/metadatos_con_gbif.tsv`.  \n",
        "**Parámetros editables:** `MANGROVE_TAXA`, `RADIUS_KM`, `MAX_ROWS`, `REQUEST_PAUSE`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66131ab7",
      "metadata": {
        "id": "66131ab7"
      },
      "source": [
        "## 4) Salidas clave y consolidación final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba2e65cf",
      "metadata": {
        "id": "ba2e65cf"
      },
      "outputs": [],
      "source": [
        "final_tsv = OUT_DIR / \"metadatos_con_gbif.tsv\"\n",
        "if final_tsv.exists():\n",
        "    df_final = pd.read_csv(final_tsv, sep=\"\\t\")\n",
        "    df_final.to_csv(META_FILE, index=False)\n",
        "    print(\"Escrito estándar:\", META_FILE)\n",
        "else:\n",
        "    print(\"No se encontró\", final_tsv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "350c9f4c",
      "metadata": {
        "id": "350c9f4c"
      },
      "source": [
        "**Resumen del bloque — Consolidación**  \n",
        "**Qué hace:** escribe la última tabla enriquecida al nombre estándar `metadatos_enriquecidos.csv`.  \n",
        "**Entradas:** `outputs/metadatos_con_gbif.tsv`.  \n",
        "**Salidas:** `outputs/metadatos_enriquecidos.csv`.  \n",
        "**Parámetros editables:** `META_BASE`, `META_FILE`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad45dec",
      "metadata": {
        "id": "2ad45dec"
      },
      "source": [
        "# Resumen de metadatos generados (esperados)\n",
        "\n",
        "Este resumen describe **qué columnas** deberían haberse añadido durante el flujo de enriquecimiento, asumiendo que todos los insumos locales y APIs estuvieron disponibles y accesibles.\n",
        "\n",
        "## Núcleo geo y filtrado\n",
        "- `latitude`, `longitude` (validadas y acotadas a rangos plausibles).\n",
        "\n",
        "## Altitud (SRTM / DEM)\n",
        "- `altitude_m` → **1 columna**  \n",
        "*Requiere `data/dem/srtm.tif` (o ajustar `DEM_TIF`).*\n",
        "\n",
        "## Clima (WorldClim 2.1, BIO1–BIO19)\n",
        "- `bio1` … `bio19` → **19 columnas**  \n",
        "*Requiere TIFFs `wc2.1_30s_bio_1..19.tif` en `data/clim/` (o ajustar `CLIM_DIR`).*\n",
        "\n",
        "## Ecorregiones marinas (MEOW)\n",
        "- `ECOREGION`, `PROVINCE`, `REALM` → **3 columnas**  \n",
        "*Requiere shapefile `meow_ecos.shp` (+ `.shx/.dbf/.prj`) en `data/marine_ecoregions/MEOW/` (o ajustar `MEOW_SHP`).*\n",
        "\n",
        "## Huella humana (HII 2020)\n",
        "- `human_footprint_2020` → **1 columna**  \n",
        "*Requiere `data/human/hii_2020.tif` (o ajustar `HII_TIF`).*\n",
        "\n",
        "## Suelos (SoilGrids v2.0, 0–30 cm)\n",
        "Propiedades: `bdod`, `cec`, `clay`, `sand`, `silt`, `phh2o`, `soc`, `nitrogen`, `cfvo`, `ocs`  \n",
        "Profundidades: `0-5cm`, `5-15cm`, `15-30cm`  \n",
        "Estadístico consolidado: `mean`  \n",
        "- Columnas esperadas: **10 propiedades × 3 profundidades = 30 columnas**  \n",
        "  Formato: `prop_depth_mean`, p.ej. `clay_0-5cm_mean`.  \n",
        "> Nota: si la API expone `Q0.5` (mediana), el flujo puede intentar leerla, pero la salida consolidada se guarda en `*_mean` cuando hay valores.\n",
        "\n",
        "## Biodiversidad (GBIF, radio 2 km)\n",
        "- `gbif_richness_species_2km` (riqueza específica)  \n",
        "- `gbif_shannon_H_2km` (diversidad de Shannon)  \n",
        "- `gbif_pielou_J_2km` (equitatividad de Pielou)  \n",
        "- `gbif_simpson_1minD_2km` (1 − índice de Simpson)  \n",
        "- `gbif_chao1_2km` (riqueza estimada, Chao1)  \n",
        "- `gbif_occurrences_effective` (número de ocurrencias contabilizadas)  \n",
        "- `gbif_top_species_effective` (Top 5 especies con conteos)  \n",
        "- `gbif_scope`, `gbif_radius_effective_m`, `gbif_spatial_mode`, `gbif_error` (diagnóstico)  \n",
        "→ **≈ 10 columnas**\n",
        "\n",
        "---\n",
        "\n",
        "## Total **adicional** estimado (si todo estuvo disponible)\n",
        "- Altitud: 1  \n",
        "- Clima: 19  \n",
        "- MEOW: 3  \n",
        "- HII: 1  \n",
        "- Suelos: 30  \n",
        "- GBIF (métricas + diagnóstico): ~10  \n",
        "**Suma mínima esperada:** **64 columnas** adicionales.\n",
        "\n",
        "> El conteo real puede variar por: presencia/ausencia de insumos locales, errores de red o cambios en APIs. En tales casos se esperan `NaN` y el flujo debería continuar (salvo pasos marcados como obligatorios por el propio notebook).\n",
        "\n",
        "## Utilidad\n",
        "- **Contexto ambiental** (clima, altitud, ecorregión, presión antrópica) por muestra.  \n",
        "- **Propiedades del suelo** (pH, textura, carbono/nitrógeno, densidad aparente) relevantes para funciones microbianas.  \n",
        "- **Biodiversidad local** (riqueza/diversidad) para relacionar composición/función metagenómica con entorno biológico.  \n",
        "- **Comparabilidad y reproducibilidad** al estandarizar metadatos para análisis multi-estudio, conservación y restauración.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Preprocesamiento de lecturas: QC → trimming → ensamblaje\n",
        "\n",
        "**Binarios requeridos en PATH:** `fastqc`, `multiqc`, `fastp`, `megahit`.\n",
        "\n",
        "**Variables de entorno opcionales:**\n",
        "- `MAG_PROJECT_DIR`: fuerza la base del proyecto (por defecto usa el `PROJECT_DIR`).\n",
        "- `MAG_RAW_DIR`: ruta a FASTQ crudos. Por defecto: `PROJECT_DIR/mags/data/raw`.\n",
        "- `MAG_QC_THREADS`, `MAG_FASTP_THREADS`, `MAG_FASTP_MAX_WORKERS`, `MAG_VALIDATE_READS`, `MAG_MEGAHIT_THREADS`.\n"
      ],
      "metadata": {
        "id": "M-7Wl-_D09-y"
      },
      "id": "M-7Wl-_D09-y"
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import os, re, sys, shutil, subprocess, csv, json, gzip\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Raíz del proyecto: MAG_PROJECT_DIR tiene prioridad, si no MAGENTA_DIR, si no cwd.\n",
        "PROJECT_DIR = Path(\n",
        "    os.environ.get(\"MAG_PROJECT_DIR\", os.environ.get(\"MAGENTA_DIR\", Path.cwd()))\n",
        ").resolve()\n",
        "\n",
        "# Estructura base tipo MAGs dentro del proyecto\n",
        "MAGS_DIR    = PROJECT_DIR / \"mags\"\n",
        "DATA_DIR    = MAGS_DIR / \"data\"            # (no se usa como default de RAW)\n",
        "RESULTS_DIR = MAGS_DIR / \"results\"\n",
        "SCRIPTS_DIR = MAGS_DIR / \"scripts\"\n",
        "\n",
        "# Subcarpetas de resultados\n",
        "QC_DIR      = RESULTS_DIR / \"01.qc\"\n",
        "TRIM_DIR    = RESULTS_DIR / \"02.trimmed\"\n",
        "ASM_DIR     = RESULTS_DIR / \"03.assembly\"\n",
        "ASM_LOG_DIR = ASM_DIR / \"logs\"\n",
        "\n",
        "# Metadatos / reportes\n",
        "PIPE_META          = RESULTS_DIR / \"pipeline_meta\"\n",
        "PIPE_META.mkdir(parents=True, exist_ok=True)\n",
        "PIPE_MANIFEST_CSV  = PIPE_META / \"pipeline_manifest.csv\"\n",
        "TRIM_REPORT_CSV    = PIPE_META / \"trim_report.csv\"\n",
        "ASM_RESUMEN_CSV    = ASM_DIR   / \"resumen_ensamblaje.csv\"\n",
        "VALID_SAMPLES_CSV  = PIPE_META / \"muestras_validas.csv\"\n",
        "\n",
        "# Donde 02_descargar_y_convertir_mangrove.py deja los FASTQ\n",
        "DEFAULT_RAW_SRC = PROJECT_DIR / \"rawdata\" / \"convertidos\"\n",
        "RAW_SRC = Path(os.environ.get(\"MAG_RAW_DIR\", DEFAULT_RAW_SRC))\n",
        "\n",
        "# Crear estructura necesaria\n",
        "for d in [MAGS_DIR, DATA_DIR, RESULTS_DIR, SCRIPTS_DIR, QC_DIR, TRIM_DIR, ASM_DIR, ASM_LOG_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Parámetros\n",
        "N_THREADS_FASTQC   = int(os.environ.get(\"MAG_QC_THREADS\",        \"12\"))\n",
        "N_THREADS_FASTP    = int(os.environ.get(\"MAG_FASTP_THREADS\",     \"12\"))\n",
        "MAX_WORKERS_FASTP  = int(os.environ.get(\"MAG_FASTP_MAX_WORKERS\", \"4\"))\n",
        "MAX_READS_VALIDATE = int(os.environ.get(\"MAG_VALIDATE_READS\",    \"10000\"))\n",
        "MEGAHIT_THREADS    = int(os.environ.get(\"MAG_MEGAHIT_THREADS\",   \"40\"))\n",
        "SAMPLE_REGEX       = os.environ.get(\n",
        "    \"MAG_SAMPLE_REGEX\",\n",
        "    r\"^([A-Za-z0-9_\\-]+)_([12])\\.fastq(?:\\.gz)?$\"\n",
        ")\n",
        "\n",
        "def have_bin(b: str) -> bool:\n",
        "    return shutil.which(b) is not None\n",
        "\n",
        "def run(cmd, **kwargs) -> int:\n",
        "    printable = \" \".join(map(str, cmd)) if isinstance(cmd, (list,tuple)) else str(cmd)\n",
        "    print(\"[CMD]\", printable)\n",
        "    return subprocess.call(cmd, shell=isinstance(cmd, str), **kwargs)\n",
        "\n",
        "print(\"[PORTABLE] PROJECT_DIR =\", PROJECT_DIR)\n",
        "print(\"[PORTABLE] RAW_SRC     =\", RAW_SRC)\n",
        "print(\"[PORTABLE] RESULTS_DIR  =\", RESULTS_DIR)"
      ],
      "metadata": {
        "id": "thMFTFqF1dfD"
      },
      "id": "thMFTFqF1dfD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rutas y estructura de directorios\n",
        "\n",
        "**Base del proyecto (PROJECT_DIR):**  \n",
        "Determinada en este orden:\n",
        "1. `MAG_PROJECT_DIR`\n",
        "2. `MAGENTA_DIR`\n",
        "3. Directorio de trabajo actual (`cwd`)\n",
        "\n",
        "**Estructura creada dentro de `PROJECT_DIR`:**\n",
        "- `mags/`\n",
        "- `data/` (no se usa como entrada por defecto de FASTQ)\n",
        "- `results/`\n",
        "  - `01.qc/` (FastQC crudo y post-trim, MultiQC)\n",
        "  - `02.trimmed/` (salidas de fastp)\n",
        "  - `03.assembly/` (ensamblajes MEGAHIT)\n",
        "- `pipeline_meta/` (manifiestos y reportes)\n",
        "- `scripts/`\n",
        "\n",
        "**Entrada por defecto de FASTQ:**  \n",
        "`rawdata/convertidos/`, generada por el script de descarga y conversión.  \n",
        "Puede sobrescribirse con la variable `MAG_RAW_DIR`.\n",
        "\n",
        "---\n",
        "\n",
        "## Parámetros configurables\n",
        "\n",
        "Los siguientes parámetros se leen de variables de entorno, con valores por defecto sensatos:\n",
        "\n",
        "- `MAG_QC_THREADS` (12) → hilos para FastQC  \n",
        "- `MAG_FASTP_THREADS` (12) → hilos para fastp  \n",
        "- `MAG_FASTP_MAX_WORKERS` (4) → número máximo de muestras en paralelo para fastp  \n",
        "- `MAG_VALIDATE_READS` (10000) → lecturas muestreadas para validación de FASTQ  \n",
        "- `MAG_MEGAHIT_THREADS` (40) → hilos para MEGAHIT  \n",
        "- `MAG_SAMPLE_REGEX` → expresión regular para identificar muestras a partir del nombre de archivo  \n",
        "\n",
        "---\n",
        "\n",
        "## Utilidades del bloque\n",
        "\n",
        "- **`have_bin(b: str)`**  \n",
        "  Comprueba si un binario requerido está disponible en el `PATH`.\n",
        "\n",
        "- **`run(cmd, **kwargs)`**  \n",
        "  Ejecuta un comando, imprime la línea para trazabilidad y devuelve el código de salida.\n",
        "\n",
        "---\n",
        "\n",
        "## Archivos de salida esperados\n",
        "\n",
        "- `pipeline_manifest.csv` → listado de muestras y rutas de FASTQ detectadas  \n",
        "- `trim_report.csv` → reporte de trimming  \n",
        "- `resumen_ensamblaje.csv` → estado de los ensamblajes por muestra  \n",
        "- `muestras_validas.csv` → resumen de pares de FASTQ validados  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ne8omp6B8LRA"
      },
      "id": "ne8omp6B8LRA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔍 5.1 Control de calidad inicial — FastQC + MultiQC\n"
      ],
      "metadata": {
        "id": "jlHNbsjX9kss"
      },
      "id": "jlHNbsjX9kss"
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# 1) Detectar pares (y single-end) a partir de SAMPLE_REGEX\n",
        "def list_pairs(raw_dir: Path, sample_regex: str) -> dict[str, dict[str, Path]]:\n",
        "    rx = re.compile(sample_regex)\n",
        "    buckets: dict[str, dict[str, Path]] = {}\n",
        "    for p in sorted(raw_dir.rglob(\"*.fastq*\")):\n",
        "        m = rx.match(p.name)\n",
        "        if not m:\n",
        "            continue\n",
        "        sample, read = m.group(1), m.group(2)\n",
        "        d = buckets.setdefault(sample, {\"R1\": None, \"R2\": None})\n",
        "        if read == \"1\" and d[\"R1\"] is None:\n",
        "            d[\"R1\"] = p\n",
        "        elif read == \"2\" and d[\"R2\"] is None:\n",
        "            d[\"R2\"] = p\n",
        "    # Aceptar single-end con nombres <sample>.fastq(.gz)\n",
        "    for p in sorted(raw_dir.rglob(\"*.fastq*\")):\n",
        "        if rx.match(p.name):\n",
        "            continue\n",
        "        base = p.name.replace(\".gz\", \"\")\n",
        "        if base.endswith(\".fastq\"):\n",
        "            sample = base[:-6]  # quita \".fastq\"\n",
        "            buckets.setdefault(sample, {\"R1\": None, \"R2\": None})\n",
        "            if buckets[sample][\"R1\"] is None:\n",
        "                buckets[sample][\"R1\"] = p\n",
        "    return buckets\n",
        "\n",
        "def write_manifest(pairs: dict[str, dict[str, Path]]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for s, rr in sorted(pairs.items()):\n",
        "        rows.append({\"sample\": s, \"R1\": str(rr.get(\"R1\") or \"\"), \"R2\": str(rr.get(\"R2\") or \"\")})\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(PIPE_MANIFEST_CSV, index=False)\n",
        "    print(f\"[META] Manifiesto muestras -> {PIPE_MANIFEST_CSV}\")\n",
        "    return df\n",
        "\n",
        "# 2) FastQC crudo (paralelo por lotes para no saturar la CLI)\n",
        "def run_fastqc(inputs: list[Path], outdir: Path, threads: int) -> None:\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "    if not inputs:\n",
        "        print(\"[FASTQC] Sin archivos de entrada.\")\n",
        "        return\n",
        "    batch = []\n",
        "    for p in inputs:\n",
        "        batch.append(str(p))\n",
        "        if len(batch) >= 64:\n",
        "            run([\"fastqc\", \"-t\", str(threads), \"-o\", str(outdir)] + batch)\n",
        "            batch = []\n",
        "    if batch:\n",
        "        run([\"fastqc\", \"-t\", str(threads), \"-o\", str(outdir)] + batch)\n",
        "\n",
        "# Ejecutar manifiesto + FastQC\n",
        "if not RAW_SRC.exists():\n",
        "    raise SystemExit(f\"[ERROR] No existe RAW_SRC: {RAW_SRC}\")\n",
        "\n",
        "pairs = list_pairs(RAW_SRC, SAMPLE_REGEX)\n",
        "if not pairs:\n",
        "    raise SystemExit(f\"[ERROR] No se detectaron FASTQ en {RAW_SRC}\")\n",
        "\n",
        "manifest_df = write_manifest(pairs)\n",
        "\n",
        "raw_fastqs = []\n",
        "for rr in pairs.values():\n",
        "    if rr.get(\"R1\"): raw_fastqs.append(rr[\"R1\"])\n",
        "    if rr.get(\"R2\"): raw_fastqs.append(rr[\"R2\"])\n",
        "\n",
        "if have_bin(\"fastqc\"):\n",
        "    run_fastqc(raw_fastqs, QC_DIR / \"raw_fastqc\", N_THREADS_FASTQC)\n",
        "    if have_bin(\"multiqc\"):\n",
        "        run([\"multiqc\", str(QC_DIR / \"raw_fastqc\"), \"-o\", str(QC_DIR / \"raw_fastqc\")])\n",
        "else:\n",
        "    print(\"[AVISO] 'fastqc' no está en PATH. Se omite QC inicial.\")"
      ],
      "metadata": {
        "id": "S1bxtCt99oMq"
      },
      "id": "S1bxtCt99oMq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque se encarga de **detectar archivos FASTQ** en el directorio de entrada (`RAW_SRC`), generar un **manifiesto de muestras** y ejecutar **FastQC en paralelo** (con soporte opcional para MultiQC).\n",
        "\n",
        "## Flujo principal\n",
        "\n",
        "1. **Descubrimiento de muestras (`list_pairs`)**\n",
        "   - Detecta pares de archivos `R1`/`R2` usando `SAMPLE_REGEX`.\n",
        "   - Admite también **single-end** con nombres `<sample>.fastq(.gz)`.\n",
        "   - Devuelve un diccionario con las rutas asociadas a cada muestra.\n",
        "\n",
        "2. **Generación de manifiesto (`write_manifest`)**\n",
        "   - Construye un `DataFrame` con columnas:\n",
        "     - `sample`\n",
        "     - `R1`\n",
        "     - `R2`\n",
        "   - Guarda el archivo `pipeline_manifest.csv`.\n",
        "   - Imprime la ruta generada.\n",
        "\n",
        "3. **Ejecución de FastQC (`run_fastqc`)**\n",
        "   - Revisa que existan archivos de entrada.\n",
        "   - Crea el directorio `QC_DIR/raw_fastqc` si no existe.\n",
        "   - Procesa los archivos en **lotes de hasta 64** para no saturar la CLI.\n",
        "   - Ejecuta:  \n",
        "     ```bash\n",
        "     fastqc -t <threads> -o <QC_DIR/raw_fastqc> <archivos>\n",
        "     ```\n",
        "     ### Parámetros en *paired-end*\n",
        "\n",
        "Cuando hay **R1** y **R2**:\n",
        "\n",
        "- `-I`: archivo de entrada **R2** (lecturas *reverse*).  \n",
        "- `-O`: archivo de salida de lecturas **R2** procesadas.  \n",
        "- `--detect_adapter_for_pe`: activa la detección automática de adaptadores para datos *paired-end*.  \n",
        "\n",
        "4. **MultiQC opcional**\n",
        "   - Si está disponible, ejecuta:  \n",
        "     ```bash\n",
        "     multiqc <QC_DIR/raw_fastqc> -o <QC_DIR/raw_fastqc>\n",
        "     ```\n",
        "---\n",
        "\n",
        "## Condiciones y validaciones\n",
        "\n",
        "- Si `RAW_SRC` no existe → termina con error.  \n",
        "- Si no se detectan FASTQ → termina con error.  \n",
        "- Si `fastqc` no está en el `PATH` → omite la etapa con aviso.  \n",
        "- Si `multiqc` no está disponible → solo se ejecuta FastQC.  \n",
        "\n",
        "## Archivos generados\n",
        "\n",
        "- `pipeline_manifest.csv` → listado de muestras y archivos FASTQ detectados.  \n",
        "- Resultados de **FastQC** en `QC_DIR/raw_fastqc/`.  \n",
        "- Reporte combinado de **MultiQC** (si está instalado).  "
      ],
      "metadata": {
        "id": "GfoGKDxI-fZt"
      },
      "id": "GfoGKDxI-fZt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Trimming de lecturas — *fastp* en paralelo"
      ],
      "metadata": {
        "id": "2Vz2OG6__qR2"
      },
      "id": "2Vz2OG6__qR2"
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import os, re, sys, shutil, subprocess, argparse, shlex\n",
        "import pandas as pd\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Rutas y parámetros base\n",
        "# ---------------------------\n",
        "PROJECT_DIR = Path(os.environ.get(\"MAG_PROJECT_DIR\", os.environ.get(\"MAGENTA_DIR\", Path.cwd()))).resolve()\n",
        "MAGS_DIR    = PROJECT_DIR / \"mags\"\n",
        "RESULTS_DIR = MAGS_DIR / \"results\"\n",
        "TRIM_DIR    = RESULTS_DIR / \"02.trimmed\"\n",
        "PIPE_META   = RESULTS_DIR / \"pipeline_meta\"\n",
        "PIPE_META.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PIPE_MANIFEST_CSV = PIPE_META / \"pipeline_manifest.csv\"\n",
        "TRIM_REPORT_CSV   = PIPE_META / \"trim_report.csv\"\n",
        "\n",
        "DEFAULT_RAW_SRC = PROJECT_DIR / \"rawdata\" / \"convertidos\"\n",
        "RAW_SRC = Path(os.environ.get(\"MAG_RAW_DIR\", DEFAULT_RAW_SRC))\n",
        "\n",
        "SAMPLE_REGEX = os.environ.get(\"MAG_SAMPLE_REGEX\", r\"^([A-Za-z0-9_\\-]+)_([12])\\.fastq(?:\\.gz)?$\")\n",
        "\n",
        "N_THREADS_FASTP = int(os.environ.get(\"MAG_FASTP_THREADS\", \"12\"))\n",
        "\n",
        "def have_bin(b: str) -> bool:\n",
        "    from shutil import which\n",
        "    return which(b) is not None\n",
        "\n",
        "def run(cmd, **kwargs) -> int:\n",
        "    printable = \" \".join(map(str, cmd)) if isinstance(cmd, (list,tuple)) else str(cmd)\n",
        "    print(\"[CMD]\", printable, flush=True)\n",
        "    return subprocess.call(cmd, shell=isinstance(cmd, str), **kwargs)\n",
        "\n",
        "print(f\"[TRIM] PROJECT_DIR = {PROJECT_DIR}\")\n",
        "print(f\"[TRIM] RAW_SRC     = {RAW_SRC}\")\n",
        "print(f\"[TRIM] TRIM_DIR    = {TRIM_DIR}\")\n",
        "\n",
        "# --------------------------------\n",
        "# 1) Cargar o descubrir 'pairs'\n",
        "# --------------------------------\n",
        "def rebuild_pairs_from_manifest(manifest_csv: Path) -> Dict[str, Dict[str, Optional[Path]]]:\n",
        "    df = pd.read_csv(manifest_csv)\n",
        "    pairs: Dict[str, Dict[str, Optional[Path]]] = {}\n",
        "    for _, row in df.iterrows():\n",
        "        s = str(row[\"sample\"])\n",
        "        r1 = Path(row[\"R1\"]) if isinstance(row[\"R1\"], str) and row[\"R1\"] else None\n",
        "        r2 = Path(row[\"R2\"]) if isinstance(row[\"R2\"], str) and row[\"R2\"] else None\n",
        "        pairs[s] = {\"R1\": r1, \"R2\": r2}\n",
        "    return pairs\n",
        "\n",
        "def discover_pairs(raw_dir: Path, sample_regex: str) -> Dict[str, Dict[str, Optional[Path]]]:\n",
        "    rx = re.compile(sample_regex)\n",
        "    buckets: Dict[str, Dict[str, Optional[Path]]] = {}\n",
        "    for p in sorted(raw_dir.rglob(\"*.fastq*\")):\n",
        "        m = rx.match(p.name)\n",
        "        if not m: continue\n",
        "        sample, read = m.group(1), m.group(2)\n",
        "        d = buckets.setdefault(sample, {\"R1\": None, \"R2\": None})\n",
        "        if read == \"1\" and d[\"R1\"] is None: d[\"R1\"] = p\n",
        "        elif read == \"2\" and d[\"R2\"] is None: d[\"R2\"] = p\n",
        "    # Single-end: <sample>.fastq(.gz)\n",
        "    for p in sorted(raw_dir.rglob(\"*.fastq*\")):\n",
        "        if rx.match(p.name): continue\n",
        "        base = p.name.replace(\".gz\", \"\")\n",
        "        if base.endswith(\".fastq\"):\n",
        "            sample = base[:-6]\n",
        "            buckets.setdefault(sample, {\"R1\": None, \"R2\": None})\n",
        "            if buckets[sample][\"R1\"] is None:\n",
        "                buckets[sample][\"R1\"] = p\n",
        "    return buckets\n",
        "\n",
        "# --------------------------------\n",
        "# 2) fastp (default, sin FastQC/MultiQC)\n",
        "# --------------------------------\n",
        "def build_fastp_cmd(R1: Path, R2: Optional[Path], r1_out: Path, r2_out: Optional[Path],\n",
        "                    threads: int, extra_opts: List[str]) -> List[str]:\n",
        "    cmd = [\"fastp\", \"-w\", str(threads), \"-o\", str(r1_out)]\n",
        "    if R2:\n",
        "        cmd += [\"-O\", str(r2_out), \"-i\", str(R1), \"-I\", str(R2), \"--detect_adapter_for_pe\"]\n",
        "    else:\n",
        "        cmd += [\"-i\", str(R1)]\n",
        "    # NOTA: NO añadimos -j/-h para no generar reportes fastp explícitos (evitar ruido).\n",
        "    # fastp aún podría crear fastp.json/html por defecto en algunos builds; se ignoran.\n",
        "    cmd += extra_opts\n",
        "    return cmd\n",
        "\n",
        "def fastp_one(sample: str, R1: Optional[Path], R2: Optional[Path],\n",
        "              threads: int, extra_opts: List[str]) -> dict:\n",
        "    TRIM_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    if not have_bin(\"fastp\"):\n",
        "        return {\"sample\": sample, \"ok\": False, \"reason\": \"fastp_not_found\"}\n",
        "    if not (R1 and Path(R1).exists()):\n",
        "        return {\"sample\": sample, \"ok\": False, \"reason\": \"no_R1\"}\n",
        "    if R2 and not Path(R2).exists():\n",
        "        return {\"sample\": sample, \"ok\": False, \"reason\": \"R2_missing\"}\n",
        "\n",
        "    r1_out = TRIM_DIR / f\"{sample}_R1.fastq.gz\"\n",
        "    r2_out = TRIM_DIR / f\"{sample}_R2.fastq.gz\" if R2 else None\n",
        "\n",
        "    code = run(build_fastp_cmd(R1, R2, r1_out, r2_out, threads, extra_opts))\n",
        "    ok = (code == 0) and r1_out.exists() and (r2_out is None or r2_out.exists())\n",
        "    return {\n",
        "        \"sample\": sample,\n",
        "        \"ok\": ok,\n",
        "        \"trimmer\": \"fastp\",\n",
        "        \"R1_out\": str(r1_out) if r1_out.exists() else \"\",\n",
        "        \"R2_out\": str(r2_out) if (r2_out and r2_out.exists()) else \"\",\n",
        "    }\n",
        "\n",
        "# --------------------------------\n",
        "# 3) Trim Galore (sin --fastqc)\n",
        "# --------------------------------\n",
        "def _find_tg_outputs_pe(R1: Path, R2: Path, outdir: Path) -> Tuple[Optional[Path], Optional[Path]]:\n",
        "    base1 = Path(R1).stem\n",
        "    base2 = Path(R2).stem\n",
        "    cand1 = list(outdir.glob(f\"*{base1}*_val_1.fq.gz\")) + list(outdir.glob(f\"*{base1}*_val_1.fastq.gz\"))\n",
        "    cand2 = list(outdir.glob(f\"*{base2}*_val_2.fq.gz\")) + list(outdir.glob(f\"*{base2}*_val_2.fastq.gz\"))\n",
        "    if not cand1:\n",
        "        cand1 = list(outdir.glob(\"*_val_1.fq.gz\")) + list(outdir.glob(\"*_val_1.fastq.gz\"))\n",
        "    if not cand2:\n",
        "        cand2 = list(outdir.glob(\"*_val_2.fq.gz\")) + list(outdir.glob(\"*_val_2.fastq.gz\"))\n",
        "    return (cand1[0] if cand1 else None, cand2[0] if cand2 else None)\n",
        "\n",
        "def _find_tg_outputs_se(R1: Path, outdir: Path) -> Optional[Path]:\n",
        "    base1 = Path(R1).stem\n",
        "    cand = list(outdir.glob(f\"*{base1}*trimmed.fq.gz\")) + list(outdir.glob(f\"*{base1}*trimmed.fastq.gz\"))\n",
        "    if not cand:\n",
        "        cand = list(outdir.glob(\"*trimmed.fq.gz\")) + list(outdir.glob(\"*trimmed.fastq.gz\"))\n",
        "    return cand[0] if cand else None\n",
        "\n",
        "def trim_galore_one(sample: str, R1: Optional[Path], R2: Optional[Path],\n",
        "                    tg_opts_pe: List[str], tg_opts_se: List[str]) -> dict:\n",
        "    TRIM_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    if not have_bin(\"trim_galore\"):\n",
        "        return {\"sample\": sample, \"ok\": False, \"reason\": \"trim_galore_not_found\"}\n",
        "    if not (R1 and Path(R1).exists()):\n",
        "        return {\"sample\": sample, \"ok\": False, \"reason\": \"no_R1\"}\n",
        "    if R2 and not Path(R2).exists():\n",
        "        return {\"sample\": sample, \"ok\": False, \"reason\": \"R2_missing\"}\n",
        "\n",
        "    # Importante: sin --fastqc y con --no_report_file para evitar QC redundante\n",
        "    base_opts = [\"--no_report_file\"]\n",
        "    if R2:\n",
        "        cmd = [\"trim_galore\", \"--paired\"] + base_opts + tg_opts_pe + [\"-o\", str(TRIM_DIR), str(R1), str(R2)]\n",
        "    else:\n",
        "        cmd = [\"trim_galore\"] + base_opts + tg_opts_se + [\"-o\", str(TRIM_DIR), str(R1)]\n",
        "    code = run(cmd)\n",
        "\n",
        "    if R2:\n",
        "        out1_src, out2_src = _find_tg_outputs_pe(R1, R2, TRIM_DIR)\n",
        "        out1_dst = TRIM_DIR / f\"{sample}_R1.fastq.gz\"\n",
        "        out2_dst = TRIM_DIR / f\"{sample}_R2.fastq.gz\"\n",
        "        if out1_src and not out1_dst.exists(): shutil.move(str(out1_src), str(out1_dst))\n",
        "        if out2_src and not out2_dst.exists(): shutil.move(str(out2_src), str(out2_dst))\n",
        "        ok = (code == 0) and out1_dst.exists() and out2_dst.exists()\n",
        "        return {\"sample\": sample, \"ok\": ok, \"trimmer\": \"trim_galore\",\n",
        "                \"R1_out\": str(out1_dst) if out1_dst.exists() else \"\",\n",
        "                \"R2_out\": str(out2_dst) if out2_dst.exists() else \"\"}\n",
        "    else:\n",
        "        out_src = _find_tg_outputs_se(R1, TRIM_DIR)\n",
        "        out_dst = TRIM_DIR / f\"{sample}_R1.fastq.gz\"\n",
        "        if out_src and not out_dst.exists(): shutil.move(str(out_src), str(out_dst))\n",
        "        ok = (code == 0) and out_dst.exists()\n",
        "        return {\"sample\": sample, \"ok\": ok, \"trimmer\": \"trim_galore\",\n",
        "                \"R1_out\": str(out_dst) if out_dst.exists() else \"\",\n",
        "                \"R2_out\": \"\"}\n",
        "\n",
        "# --------------------------------\n",
        "# 4) Main\n",
        "# --------------------------------\n",
        "def main(argv: Optional[List[str]] = None) -> int:\n",
        "    parser = argparse.ArgumentParser(description=\"Trimming con fastp (default) o Trim Galore, sin QC post-trimming.\")\n",
        "    parser.add_argument(\"--trimmer\", choices=[\"fastp\",\"trim_galore\"], default=os.environ.get(\"MAG_TRIM_TOOL\",\"fastp\"),\n",
        "                        help=\"Herramienta de trimming (default: fastp).\")\n",
        "    parser.add_argument(\"--threads-fastp\", type=int, default=N_THREADS_FASTP, help=\"Hilos para fastp.\")\n",
        "    parser.add_argument(\"--fastp-opts\", type=str, default=os.environ.get(\"MAG_FASTP_OPTS\",\"\"),\n",
        "                        help=\"Flags extra para fastp (string estilo shell).\")\n",
        "    parser.add_argument(\"--tg-pe-opts\", type=str,\n",
        "                        default=os.environ.get(\"MAG_TRIMGALORE_PE\",\"--cores 20 --quality 15 --length 50\"),\n",
        "                        help=\"Flags Trim Galore para paired-end (sin --fastqc).\")\n",
        "    parser.add_argument(\"--tg-se-opts\", type=str,\n",
        "                        default=os.environ.get(\"MAG_TRIMGALORE_SE\",\"--cores 20 --quality 15 --length 50\"),\n",
        "                        help=\"Flags Trim Galore para single-end (sin --fastqc).\")\n",
        "    args = parser.parse_args(argv)\n",
        "\n",
        "    TRIM_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Pairs desde manifiesto o descubrimiento\n",
        "    if PIPE_MANIFEST_CSV.exists():\n",
        "        pairs = rebuild_pairs_from_manifest(PIPE_MANIFEST_CSV)\n",
        "        print(f\"[TRIM] Cargado pairs desde {PIPE_MANIFEST_CSV} ({len(pairs)} muestras).\")\n",
        "    else:\n",
        "        if not RAW_SRC.exists():\n",
        "            print(f\"[ERROR] No existe RAW_SRC: {RAW_SRC}\", file=sys.stderr)\n",
        "            return 2\n",
        "        pairs = discover_pairs(RAW_SRC, SAMPLE_REGEX)\n",
        "        print(f\"[TRIM] Descubierto pairs escaneando {RAW_SRC} ({len(pairs)} muestras).\")\n",
        "    if not pairs:\n",
        "        print(\"[ERROR] No hay muestras detectadas.\", file=sys.stderr)\n",
        "        return 3\n",
        "\n",
        "    reports: List[dict] = []\n",
        "    for sample, rr in sorted(pairs.items()):\n",
        "        R1, R2 = rr.get(\"R1\"), rr.get(\"R2\")\n",
        "        if args.trimmer == \"fastp\":\n",
        "            extra = shlex.split(args.fastp_opts) if args.fastp_opts else []\n",
        "            rep = fastp_one(sample, R1, R2, threads=args.threads_fastp, extra_opts=extra)\n",
        "        else:\n",
        "            tg_pe = shlex.split(args.tg_pe_opts) if args.tg_pe_opts else []\n",
        "            tg_se = shlex.split(args.tg_se_opts) if args.tg_se_opts else []\n",
        "            rep = trim_galore_one(sample, R1, R2, tg_pe, tg_se)\n",
        "        reports.append(rep)\n",
        "\n",
        "    pd.DataFrame(reports).sort_values(\"sample\").to_csv(TRIM_REPORT_CSV, index=False)\n",
        "    print(f\"[TRIM] Reporte -> {TRIM_REPORT_CSV}\")\n",
        "    print(\"[TRIM] Finalizado (sin QC post-trim: usar 04_fastqc_parallel.py).\")\n",
        "    return 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sys.exit(main())"
      ],
      "metadata": {
        "id": "WhbAXheR_vuG"
      },
      "id": "WhbAXheR_vuG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementa un módulo de **trimming de lecturas** para un pipeline de ensamblaje de MAGs.  \n",
        "Es compatible con dos herramientas de recorte de secuencias:\n",
        "\n",
        "- **fastp** (por defecto)  \n",
        "- **Trim Galore**\n",
        "\n",
        "Su propósito es limpiar las lecturas crudas, generar archivos de salida filtrados y producir un reporte de trimming.\n",
        "\n",
        "1. Configura rutas de proyecto, resultados y directorios de calidad (`QC_DIR`, `TRIM_DIR`, `PIPE_META`).  \n",
        "2. Carga un **manifiesto de muestras** (`pipeline_manifest.csv`) con la información de las lecturas.  \n",
        "3. Ejecuta el trimming con la herramienta seleccionada:\n",
        "   - **fastp**: usando hilos definidos (`N_THREADS_FASTP`) y soporte para *single-end* o *paired-end*.  \n",
        "   - **Trim Galore**: ajustando automáticamente según el tipo de datos (single o paired).  \n",
        "4. Guarda un reporte (`trim_report.csv`) con el detalle de los archivos generados por cada muestra.  \n",
        "\n",
        "## Inputs\n",
        "- **Archivo de manifiesto** (`pipeline_manifest.csv`), con columnas:  \n",
        "  - `sample`: nombre de la muestra  \n",
        "  - `R1`: archivo de lecturas forward  \n",
        "  - `R2`: archivo de lecturas reverse (opcional, si es paired-end)  \n",
        "- Variables de entorno (opcionales):  \n",
        "  - `MAG_PROJECT_DIR` o `MAGENTA_DIR`: define el directorio base del proyecto.  \n",
        "  - `MAG_FASTP_THREADS`: número de hilos para `fastp` (default: 12).  \n",
        "  - `MAG_FASTP_MAX_WORKERS`: número máximo de procesos paralelos (default: 4).  \n",
        "  - `MAG_TRIM_TOOL`: herramienta de trimming (`fastp` o `trim_galore`).  \n",
        "\n",
        "## Outputs\n",
        "- Archivos **FASTQ recortados**, ubicados en `02.trimmed/<sample>/`:  \n",
        "  - `<sample>_R1.trimmed.fastq.gz`  \n",
        "  - `<sample>_R2.trimmed.fastq.gz` (si aplica)  \n",
        "- **Reporte de trimming** (`trim_report.csv`) con columnas:  \n",
        "  - `sample` → nombre de muestra  \n",
        "  - `tool` → herramienta usada (`fastp` o `trim_galore`)  \n",
        "  - `R1` → archivo trimmed forward  \n",
        "  - `R2` → archivo trimmed reverse (si aplica)  \n",
        "\n",
        "## Parámetros usados\n",
        "\n",
        "### Trim Galore\n",
        "Parámetros mínimos aplicados en el flujo:\n",
        "\n",
        "- `--paired` → para lecturas pareadas  \n",
        "- `--no_report_file` → desactiva reportes por muestra (se usa MultiQC después)  \n",
        "- `--cores 20` → número de hilos  \n",
        "- `--quality 15` → trimming de bases con calidad < Q15  \n",
        "- `--length 50` → descarta lecturas post-trimming con longitud < 50 nt  \n",
        "- `--fastqc` → corre FastQC por muestra  \n",
        "\n",
        "### fastp (por defecto)\n",
        "\n",
        "- `-q 15` / `--qualified_quality_phred 15` → calidad mínima por base (Q ≥ 15)  \n",
        "- `-u 40` / `--unqualified_percent_limit 40` → descarta la lectura si >40% de bases < Q15  \n",
        "- `-l 15` / `--length_required 15` → descarta lecturas más cortas que 15 nt  \n",
        "- `--detect_adapter_for_pe` → detección y recorte automático de adaptadores (paired-end)  \n",
        "- **Poly-G trimming** → activado automáticamente en datos de NovaSeq/NextSeq  \n",
        "\n",
        "- **Trim Galore** en el pipeline está configurado para ser más estricto con la longitud (≥50 nt) y siempre corre FastQC.  \n",
        "- **fastp**, por defecto, aplica filtros de calidad equivalentes (Q15) y longitud mínima (≥15 nt), además de recorte automático de adaptadores.  "
      ],
      "metadata": {
        "id": "uCH4hWTW_16d"
      },
      "id": "uCH4hWTW_16d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Ensamblaje metagenómico (MEGAHIT por defecto, MetaSPAdes opcional)"
      ],
      "metadata": {
        "id": "Kc3nclYlIBxU"
      },
      "id": "Kc3nclYlIBxU"
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import os, sys, csv, shutil, subprocess, argparse\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "# ==========\n",
        "# Rutas base\n",
        "# ==========\n",
        "PROJECT_DIR = Path(os.environ.get(\"MAG_PROJECT_DIR\", os.environ.get(\"MAGENTA_DIR\", Path.cwd()))).resolve()\n",
        "MAGS_DIR    = PROJECT_DIR / \"mags\"\n",
        "RESULTS_DIR = MAGS_DIR / \"results\"\n",
        "\n",
        "TRIM_DIR    = RESULTS_DIR / \"02.trimmed\"\n",
        "ASM_DIR     = RESULTS_DIR / \"03.assembly\"\n",
        "ASM_LOG_DIR = ASM_DIR / \"logs\"\n",
        "RESUMEN_CSV = ASM_DIR / \"resumen_ensamblaje.csv\"\n",
        "\n",
        "ASM_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ==========\n",
        "# Utilidades\n",
        "# ==========\n",
        "def have_bin(b: str) -> bool:\n",
        "    from shutil import which\n",
        "    return which(b) is not None\n",
        "\n",
        "def run(cmd, log_file: Path | None = None) -> int:\n",
        "    printable = \" \".join(map(str, cmd))\n",
        "    print(\"[CMD]\", printable, flush=True)\n",
        "    if log_file:\n",
        "        with open(log_file, \"a\") as log:\n",
        "            log.write(f\"[CMD] {printable}\\n\")\n",
        "            return subprocess.call(cmd, stdout=log, stderr=log)\n",
        "    else:\n",
        "        return subprocess.call(cmd)\n",
        "\n",
        "def size_sum(f1: Path, f2: Path) -> int:\n",
        "    try:\n",
        "        return f1.stat().st_size + f2.stat().st_size\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "# =========================\n",
        "# Descubrimiento de pares\n",
        "# =========================\n",
        "R1_GLOBS_DEFAULT = [\n",
        "    \"*_R1*.fastq.gz\", \"*_1*.fastq.gz\", \"*R1*.fastq.gz\", \"*1*.fastq.gz\",\n",
        "    \"*_R1*.fq.gz\",    \"*_1*.fq.gz\",    \"*R1*.fq.gz\",    \"*1*.fq.gz\",\n",
        "]\n",
        "TOKEN_PAIRS = [(\"_R1\", \"_R2\"), (\"_1\", \"_2\"), (\"R1\", \"R2\"), (\"1\", \"2\")]\n",
        "\n",
        "def _try_match_r2_by_tokens(r1: Path) -> Optional[Path]:\n",
        "    \"\"\"Intenta encontrar R2 reemplazando tokens comunes en el nombre de R1, en el mismo directorio.\"\"\"\n",
        "    name = r1.name\n",
        "    for t1, t2 in TOKEN_PAIRS:\n",
        "        if t1 in name:\n",
        "            cand = r1.with_name(name.replace(t1, t2, 1))\n",
        "            if cand.exists():\n",
        "                return cand\n",
        "    return None\n",
        "\n",
        "def _best_effort_r2_in_dir(r1: Path) -> Optional[Path]:\n",
        "    \"\"\"\n",
        "    Si el reemplazo directo falla, intenta localizar un R2 plausible en el mismo directorio:\n",
        "    - Algún archivo que contenga un token R2 y termine en .f*q.gz\n",
        "    - Preferir el que comparte mayor prefijo con R1.\n",
        "    \"\"\"\n",
        "    dirp = r1.parent\n",
        "    candidates: List[Path] = []\n",
        "    for patt in [\"*_R2*.fastq.gz\", \"*_2*.fastq.gz\", \"*R2*.fastq.gz\", \"*2*.fastq.gz\",\n",
        "                 \"*_R2*.fq.gz\",   \"*_2*.fq.gz\",   \"*R2*.fq.gz\",   \"*2*.fq.gz\"]:\n",
        "        candidates.extend(dirp.glob(patt))\n",
        "    if not candidates:\n",
        "        return None\n",
        "\n",
        "    def common_prefix_len(a: str, b: str) -> int:\n",
        "        n = min(len(a), len(b)); i = 0\n",
        "        while i < n and a[i] == b[i]:\n",
        "            i += 1\n",
        "        return i\n",
        "\n",
        "    best = max(candidates, key=lambda p: common_prefix_len(p.name, r1.name))\n",
        "    return best\n",
        "\n",
        "def _collect_sample_dirs(root: Path) -> List[Path]:\n",
        "    \"\"\"Devuelve subdirectorios de primer nivel si existen; si no, devuelve [root] para fallback plano.\"\"\"\n",
        "    subdirs = [d for d in sorted(root.iterdir()) if d.is_dir()]\n",
        "    return subdirs if subdirs else [root]\n",
        "\n",
        "def find_pairs(trim_root: Path, debug: bool=False) -> List[Tuple[str, Path, Path]]:\n",
        "    \"\"\"\n",
        "    Busca pares R1/R2 en estructura:\n",
        "      trim_root/<sample>/**/{R1,R2}*.f*q.gz\n",
        "    - Explora recursivamente debajo de cada subcarpeta de muestra.\n",
        "    - Acepta múltiples patrones de nombres.\n",
        "    Devuelve: lista de (sample, R1, R2) con sample = nombre del subdirectorio de primer nivel\n",
        "              o, en fallback plano, el prefijo derivado del archivo R1.\n",
        "    \"\"\"\n",
        "    pairs: List[Tuple[str, Path, Path]] = []\n",
        "    sample_dirs = _collect_sample_dirs(trim_root)\n",
        "\n",
        "    for sample_dir in sample_dirs:\n",
        "        if sample_dir == trim_root and sample_dir.is_dir() and not any(sample_dir.iterdir()):\n",
        "            continue\n",
        "\n",
        "        sample_name = sample_dir.name if sample_dir != trim_root else None\n",
        "\n",
        "        r1_candidates: List[Path] = []\n",
        "        for patt in R1_GLOBS_DEFAULT:\n",
        "            r1_candidates.extend(sample_dir.rglob(patt))\n",
        "        r1_candidates = sorted(set(r1_candidates))\n",
        "\n",
        "        if debug:\n",
        "            print(f\"[DEBUG] Dir muestra: {sample_dir}\")\n",
        "            for r1 in r1_candidates:\n",
        "                print(f\"[DEBUG]   R1 cand: {r1}\")\n",
        "\n",
        "        used: set[Path] = set()\n",
        "        for r1 in r1_candidates:\n",
        "            if r1 in used:\n",
        "                continue\n",
        "            r2 = _try_match_r2_by_tokens(r1)\n",
        "            if r2 is None or not r2.exists():\n",
        "                r2 = _best_effort_r2_in_dir(r1)\n",
        "            if r2 and r2.exists() and r2 not in used:\n",
        "                used.add(r1); used.add(r2)\n",
        "                if sample_dir == trim_root:\n",
        "                    sname = r1.name\n",
        "                    for suf in [\"_R1.fastq.gz\",\"_1.fastq.gz\",\"R1.fastq.gz\",\"1.fastq.gz\",\n",
        "                                \"_R1.fq.gz\",\"_1.fq.gz\",\"R1.fq.gz\",\"1.fq.gz\"]:\n",
        "                        if sname.endswith(suf):\n",
        "                            sname = sname[:-len(suf)]\n",
        "                            break\n",
        "                else:\n",
        "                    sname = sample_name\n",
        "                pairs.append((sname, r1, r2))\n",
        "\n",
        "        if debug and not pairs:\n",
        "            print(f\"[DEBUG]   No se encontraron pares en {sample_dir}\")\n",
        "\n",
        "    return pairs\n",
        "\n",
        "# =========================\n",
        "# Normalización de salidas\n",
        "# =========================\n",
        "def normalize_and_prefix_outputs_megahit(sample: str, out_dir: Path, log: Path):\n",
        "    \"\"\"\n",
        "    MEGAHIT produce final.contigs.fa.\n",
        "    Crear contigs.fasta y renombrar a <sample>_contigs.fasta para uniformidad.\n",
        "    \"\"\"\n",
        "    src = out_dir / \"final.contigs.fa\"\n",
        "    mid = out_dir / \"contigs.fasta\"\n",
        "    if src.exists():\n",
        "        try:\n",
        "            shutil.copyfile(src, mid)\n",
        "        except Exception as e:\n",
        "            with open(log, \"a\") as lg:\n",
        "                lg.write(f\"[WARN] No se pudo copiar a contigs.fasta: {e}\\n\")\n",
        "    for fname in [\"contigs.fasta\", \"scaffolds.fasta\"]:\n",
        "        source = out_dir / fname\n",
        "        if source.exists():\n",
        "            renamed = out_dir / f\"{sample}_{fname}\"\n",
        "            if not renamed.exists():\n",
        "                source.rename(renamed)\n",
        "            print(f\"[RENAME] {renamed.name}\")\n",
        "\n",
        "def prefix_outputs_metaspades(sample: str, out_dir: Path):\n",
        "    \"\"\"\n",
        "    MetaSPAdes produce contigs.fasta y scaffolds.fasta.\n",
        "    Renombrar con prefijo de muestra.\n",
        "    \"\"\"\n",
        "    for fname in [\"contigs.fasta\", \"scaffolds.fasta\"]:\n",
        "        source = out_dir / fname\n",
        "        if source.exists():\n",
        "            renamed = out_dir / f\"{sample}_{fname}\"\n",
        "            if not renamed.exists():\n",
        "                source.rename(renamed)\n",
        "            print(f\"[RENAME] {renamed.name}\")\n",
        "\n",
        "# ==========\n",
        "# Ensamblaje\n",
        "# ==========\n",
        "def assemble_one(sample: str, R1: Path, R2: Path, assembler: str, threads: int, mem_gb: int,\n",
        "                 overwrite: bool=False) -> tuple[str, str, str]:\n",
        "    sample_outdir = ASM_DIR / f\"{sample}_assembled\"\n",
        "    log_file = ASM_LOG_DIR / f\"{sample}.log\"\n",
        "\n",
        "    # Manejo del directorio de salida\n",
        "    if sample_outdir.exists():\n",
        "        if overwrite:\n",
        "            try:\n",
        "                shutil.rmtree(sample_outdir)\n",
        "            except Exception as e:\n",
        "                return (sample, \"ERROR\", f\"No se pudo borrar salida previa: {e}\")\n",
        "        else:\n",
        "            return (sample, \"SKIPPED\", \"Ya existe (use --overwrite para rehacer)\")\n",
        "\n",
        "    try:\n",
        "        if assembler == \"megahit\":\n",
        "            if not have_bin(\"megahit\"):\n",
        "                return (sample, \"ERROR\", \"megahit no encontrado en PATH\")\n",
        "            cmd = [\n",
        "                \"megahit\",\n",
        "                \"-1\", str(R1),\n",
        "                \"-2\", str(R2),\n",
        "                \"-o\", str(sample_outdir),   # dejar que megahit cree la carpeta\n",
        "                \"-t\", str(threads),\n",
        "                \"--presets\", \"meta-sensitive\",\n",
        "                # Correcciones añadidas (flags extra de ejemplo):\n",
        "                \"--min-contig-len\", \"1000\",\n",
        "                \"--k-list\", \"21,29,39,59,79,99,119\",\n",
        "            ]\n",
        "            code = run(cmd, log_file)\n",
        "            if code != 0:\n",
        "                try:\n",
        "                    print(f\"[LOG TAIL] {log_file}\")\n",
        "                    os.system(f\"tail -n 200 {log_file}\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "                return (sample, \"ERROR\", f\"megahit exit code {code}\")\n",
        "            normalize_and_prefix_outputs_megahit(sample, sample_outdir, log_file)\n",
        "\n",
        "        elif assembler == \"metaspades\":\n",
        "            exe = \"metaspades.py\" if have_bin(\"metaspades.py\") else (\"spades.py\" if have_bin(\"spades.py\") else None)\n",
        "            if exe is None:\n",
        "                return (sample, \"ERROR\", \"metaspades.py no encontrado en PATH\")\n",
        "            cmd = [exe, \"-1\", str(R1), \"-2\", str(R2), \"-o\", str(sample_outdir), \"-t\", str(threads)]\n",
        "            if mem_gb and int(mem_gb) > 0:\n",
        "                cmd += [\"-m\", str(int(mem_gb))]\n",
        "            # Correcciones añadidas (flag extra de ejemplo):\n",
        "            cmd += [\"--only-assembler\"]  # si ya tienes corrección hecha\n",
        "            # cmd += [\"--meta\"]  # (metaspades.py ya implica modo meta)\n",
        "            code = run(cmd, log_file)\n",
        "            if code != 0:\n",
        "                try:\n",
        "                    print(f\"[LOG TAIL] {log_file}\")\n",
        "                    os.system(f\"tail -n 200 {log_file}\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "                return (sample, \"ERROR\", f\"metaspades exit code {code}\")\n",
        "            prefix_outputs_metaspades(sample, sample_outdir)\n",
        "\n",
        "        else:\n",
        "            return (sample, \"ERROR\", f\"Assembler no soportado: {assembler}\")\n",
        "\n",
        "        return (sample, \"OK\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            print(f\"[LOG TAIL] {log_file}\")\n",
        "            os.system(f\"tail -n 200 {log_file}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        return (sample, \"ERROR\", str(e))\n",
        "\n",
        "def obtener_muestras_pendientes(pairs: List[Tuple[str, Path, Path]]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Filtra muestras sin carpeta de ensamblaje y las ordena por tamaño total ascendente.\n",
        "    \"\"\"\n",
        "    info = []\n",
        "    for sample, r1, r2 in pairs:\n",
        "        outfolder = ASM_DIR / f\"{sample}_assembled\"\n",
        "        if not outfolder.exists():\n",
        "            sz = size_sum(r1, r2)\n",
        "            info.append((sample, sz))\n",
        "    return [m for m, _ in sorted(info, key=lambda x: x[1])]\n",
        "\n",
        "# ==========\n",
        "# Main\n",
        "# ==========\n",
        "def main(argv=None) -> int:\n",
        "    parser = argparse.ArgumentParser(description=\"Ensamblaje metagenómico en serie (MEGAHIT/MetaSPAdes) con 02.trimmed/<sample>/**/R1,R2.\")\n",
        "    parser.add_argument(\"--assembler\", choices=[\"megahit\",\"metaspades\"],\n",
        "                        default=os.environ.get(\"MAG_ASSEMBLER\", \"megahit\"),\n",
        "                        help=\"Algoritmo de ensamblaje (default: megahit).\")\n",
        "    parser.add_argument(\"--threads\", type=int,\n",
        "                        default=int(os.environ.get(\"MAG_MEGAHIT_THREADS\", os.environ.get(\"MAG_SPADES_THREADS\", \"40\"))),\n",
        "                        help=\"Hilos por muestra (default: env o 40).\")\n",
        "    parser.add_argument(\"--mem-gb\", type=int,\n",
        "                        default=int(os.environ.get(\"MAG_SPADES_MEM_GB\", \"0\")),\n",
        "                        help=\"Memoria (GB) para MetaSPAdes; 0=auto.\")\n",
        "    parser.add_argument(\"--debug\", action=\"store_true\",\n",
        "                        help=\"Imprime información de depuración del descubrimiento de pares.\")\n",
        "    parser.add_argument(\"--overwrite\", action=\"store_true\",\n",
        "                        help=\"Si existe el directorio de salida de la muestra, lo elimina y vuelve a ensamblar.\")\n",
        "    args = parser.parse_args(argv)\n",
        "\n",
        "    if not TRIM_DIR.exists():\n",
        "        print(f\"[ERROR] No existe carpeta de trimmed: {TRIM_DIR}\", file=sys.stderr)\n",
        "        return 2\n",
        "\n",
        "    pairs = find_pairs(TRIM_DIR, debug=args.debug)\n",
        "    if not pairs:\n",
        "        print(f\"[ERROR] No se detectaron pares R1/R2 dentro de subdirectorios en {TRIM_DIR}\", file=sys.stderr)\n",
        "        if not args.debug:\n",
        "            print(\"Sugerencia: ejecute nuevamente con --debug para imprimir candidatos encontrados.\", file=sys.stderr)\n",
        "        return 3\n",
        "\n",
        "    pendientes = obtener_muestras_pendientes(pairs)\n",
        "    print(f\" Muestras detectadas: {len(pairs)}\")\n",
        "    print(f\" Muestras a ensamblar (ordenadas por tamaño): {len(pendientes)}\")\n",
        "    print(f\" Iniciando ensamblaje en serie con {args.threads} hilos por muestra usando '{args.assembler}'...\\n\")\n",
        "\n",
        "    idx = {s: (r1, r2) for s, r1, r2 in pairs}\n",
        "\n",
        "    resultados = []\n",
        "    for sample in pendientes:\n",
        "        R1, R2 = idx[sample]\n",
        "        print(f\" Ensamblando: {sample}\")\n",
        "        res = assemble_one(sample, R1, R2, args.assembler, args.threads, args.mem_gb, overwrite=args.overwrite)\n",
        "        resultados.append(res)\n",
        "        print(f\" Resultado: {res[1]} - {res[2]}\\n\")\n",
        "\n",
        "    ASM_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    with open(RESUMEN_CSV, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Muestra\", \"Estado\", \"Detalle\"])\n",
        "        writer.writerows(resultados)\n",
        "\n",
        "    print(f\"\\n Logs por muestra en: {ASM_LOG_DIR}\")\n",
        "    print(f\" Contigs generados en: {ASM_DIR}\")\n",
        "    print(f\" Resumen CSV: {RESUMEN_CSV}\")\n",
        "    print(\"\\n Ensamblajes completados.\")\n",
        "    return 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sys.exit(main())\n"
      ],
      "metadata": {
        "id": "FtP626DPIaw2"
      },
      "id": "FtP626DPIaw2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensamblaje metagenómico (MEGAHIT / MetaSPAdes)\n",
        "\n",
        "Ensamblar muestras **paired-end** a partir de lecturas *trimmed*, procesando **una muestra a la vez**, puedes paralelizar este proceso.  \n",
        "Usa **MEGAHIT** por defecto y **MetaSPAdes** como alternativa.\n",
        "\n",
        "1. **Localiza pares R1/R2** dentro de `mags/results/02.trimmed/<sample>/**/{R1,R2}*.f*q.gz` con patrones y reemplazo de tokens.\n",
        "2. **Ordena** las muestras pendientes por **tamaño total** (R1+R2) ascendente.\n",
        "3. **Ensamblaje** por muestra:\n",
        "   - **MEGAHIT** (default) con `--presets meta-sensitive` **y** flags adicionales sugeridos.\n",
        "   - **MetaSPAdes** (`metaspades.py` o `spades.py`) con `--only-assembler` como ejemplo extra.\n",
        "4. **Normaliza salidas**:\n",
        "   - MEGAHIT: `final.contigs.fa → contigs.fasta → <sample>_contigs.fasta`.\n",
        "   - MetaSPAdes: renombra `contigs.fasta`/`scaffolds.fasta` a `<sample>_*`.\n",
        "5. **Registra logs** por muestra y compone un **resumen CSV**.\n",
        "\n",
        "## Entradas (inputs)\n",
        "- Directorio con lecturas **recortadas**: `mags/results/02.trimmed/`\n",
        "- Archivos esperados:\n",
        "  - `<sample>/**/R1*.f*q.gz` y `<sample>/**/R2*.f*q.gz`\n",
        "- **Variables de entorno** (opcionales):\n",
        "  - `MAG_PROJECT_DIR` o `MAGENTA_DIR`: raíz del proyecto (default: `cwd`).\n",
        "  - `MAG_ASSEMBLER`: `megahit` o `metaspades` (si no se usa `--assembler`).\n",
        "  - `MAG_MEGAHIT_THREADS` / `MAG_SPADES_THREADS`: hilos por muestra (fallback: 40).\n",
        "  - `MAG_SPADES_MEM_GB`: memoria (GB) para MetaSPAdes (0 = auto).\n",
        "\n",
        "## Salidas (outputs)\n",
        "- Ensamblajes por muestra: `mags/results/03.assembly/<sample>_assembled/`\n",
        "  - **MEGAHIT**: `<sample>_contigs.fasta` (a partir de `final.contigs.fa`)\n",
        "  - **MetaSPAdes**: `<sample>_contigs.fasta` y opcional `<sample>_scaffolds.fasta`\n",
        "- **Logs**: `mags/results/03.assembly/logs/<sample>.log`\n",
        "- **Resumen**: `mags/results/03.assembly/resumen_ensamblaje.csv`\n",
        "  - Columnas: `Muestra`, `Estado` (`OK`, `ERROR`, `SKIPPED`), `Detalle` (timestamp o mensaje)\n",
        "\n",
        "## Descubrimiento de pares (R1/R2)\n",
        "- Patrones R1 (recursivo): `*_R1*.fastq.gz`, `*_1*.fq.gz`, `*R1*.fq.gz`, etc.\n",
        "- R2 por **reemplazo de tokens** (`_R1↔_R2`, `_1↔_2`, `R1↔R2`, `1↔2`) o **búsqueda mejor-esfuerzo** en el mismo directorio (prefiere mayor prefijo compartido).\n",
        "- Fallback plano: si no hay subcarpetas en `02.trimmed`, deriva el nombre de la muestra del prefijo del archivo.\n",
        "\n",
        "---\n",
        "\n",
        "## Ensambladores y **parámetros usados por el script**\n",
        "\n",
        "### Parámetros — MEGAHIT\n",
        "\n",
        "- `-1`, `-2` → FASTQ pareados.  \n",
        "- `-o <sample_outdir>` → carpeta de salida (`<sample>_assembled`).  \n",
        "- `-t <threads>` → hilos por muestra.  \n",
        "- `--presets meta-sensitive` → preset sensible para metagenomas.  \n",
        "- `--min-contig-len 1000` → **nuevo**: contigs mínimos a reportar (ejemplo didáctico).  \n",
        "- `--k-list 21,29,39,59,79,99,119` → **nuevo**: lista de *k-mers* (ejemplo didáctico).  \n",
        "\n",
        "Puedes ajustar `--min-contig-len` y `--k-list` según cobertura/heterogeneidad.  \n",
        "\n",
        "---\n",
        "\n",
        "### Parámetros — MetaSPAdes\n",
        "\n",
        "- `-1`, `-2` → FASTQ pareados.  \n",
        "- `-o <sample_outdir>` → carpeta de salida (`<sample>_assembled`).  \n",
        "- `-t <threads>` → hilos por muestra.  \n",
        "- `-m <mem_gb>` → **opcional** si `--mem-gb > 0`; si `0`, deja el autoajuste.  \n",
        "- `--only-assembler` → **nuevo**: omite la fase de corrección si ya hiciste QC/corrección aguas arriba.  \n",
        "\n",
        "## Ejemplos prácticos\n",
        "\n",
        "### MEGAHIT con contigs ≥1 kb y k-list amplio\n",
        "```bash\n",
        "python 06_assembly_serial.py --assembler megahit --threads 48\n",
        "(El script ya incluye --min-contig-len 1000 y --k-list 21,29,39,59,79,99,119 como ejemplo didáctico.)\n",
        "\n",
        "MetaSPAdes con 64 hilos, 512 GB y solo ensamblado\n",
        "python 06_assembly_serial.py --assembler metaspades --threads 64 --mem-gb 512\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KKXuChi9IlaH"
      },
      "id": "KKXuChi9IlaH"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}